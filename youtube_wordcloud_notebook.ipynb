{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Transcripts from YouTube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re\n",
    "\n",
    "def get_youtube_transcripts(video_ids):\n",
    "    \"\"\"\n",
    "    Fetches and concatenates transcripts from a list of YouTube video IDs.\n",
    "    \n",
    "    Args:\n",
    "    video_ids (list): List of YouTube video IDs as strings.\n",
    "    \n",
    "    Returns:\n",
    "    str: Concatenated transcript text.\n",
    "    \"\"\"\n",
    "    transcript_text = ''\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            # Fetch transcript\n",
    "            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            # Concatenate transcript texts\n",
    "            transcript_text += ' '.join([transcript['text'] for transcript in transcript_list]) + ' '\n",
    "            transcript_text = transcript_text.lower()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript for video ID {video_id}: {e}\")\n",
    "    \n",
    "    \n",
    "    return transcript_text\n",
    "\n",
    "\n",
    "#List your youtube video ids from its URL:\n",
    "video_ids = ['UfbyzK488Hk', 'uB0n4IZmS34']\n",
    "transcripts = get_youtube_transcripts(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix jargon and other things that get fouled up when youtube translates human speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databricks_variations = [\n",
    "    r'\\bdata breaks\\b', r'\\bdata bricks\\b', r'\\bdata brick\\b', r'\\bdata breakes\\b', r'\\bdata bri\\b', r'\\bdata brak\\b', r'\\bdata braks\\b',\n",
    "    r'\\bdatabrick\\b', r'\\bdatabriks\\b', r'\\bdata briks\\b', r'\\bdata breakers\\b', r'\\bdata brakes\\b', r'\\bdatab bricks\\b',\n",
    "    r'\\bdata brake\\b', r'\\bdata break\\b', r'\\bdatab briak\\b', r'\\bdatab briaks\\b', r'\\bdatab brick\\b', r'\\bdatab bricks\\b',\n",
    "    r'\\bdata brecks\\b', r'\\bdata briak\\b', r'\\bdata brook\\b', r'\\bdata bir\\b', r\"\\bdata burk's\\b\", r'\\bdata brs\\b'\n",
    "]\n",
    "for variation in databricks_variations:\n",
    "    transcripts = re.sub(variation, 'Databricks', transcripts, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what your [music] [applause] [music] oh [music] [music] [music] [applause] [music] welcome to the stage Databricks co-founder and ceo ali [music] [applause] gody hello hi hi everybody super excited to be here um this is my favorite week every year okay 52 weeks this is the favorite one oh wow lots of people still coming in all right so we are super excited to welcome here everyone um this is a global event in fact i think this is the largest data and ai gathering in the world right now uh and we're super excited to have you here thank you for coming uh okay so let's look at some stats these stats are awesome um so we have over 60,000 people worldwide actually watching we have 16,000 people here uh in fact we've taken over all of mcone you know north south west all of it uh we have 140 countries represented that's more countries that i will ever visit probably and i don't know maybe some of you have visited that many but uh we have 600 training sessions so lots and lots of people again training in data and ai yesterday today and that's super super awesome over 200 teams are flying in they're nervous they're going to present what they've been working on all year you know all eyes are on data and ai right so everybody wants to know what are the use cases is this stuff working is it in production so uh it's going to be super awesome you're going to listen to these talks there's 143 exhibitors please go to the expo hall and check out what they're doing uh they have a lot of stuff a lot of products so i'm going to go check it out as well super excited about that also this community has contributed to a lot of open source projects and to open source uh so this year we're 11 years in to the spark project and we have over a billion downloads now a year and same thing with the delta lake project over a billion downloads a year and mlflow which is a machine learning gen ops uh platform uh has now over 200 million downloads and then we wanted to run a fun stat so we wanted to check how many lines of code has datab employees contributed to open source and you can see here it's 12 million actually actually this one was hard to compute took us a long time initially we thought we contributed 100 million lines uh but then we ran the stats carefully and turns out a lot of it is just generated code okay so you have to figure that out so just put it in perspective i think the whole android project is about three million lines of code um okay so this is obviously we're all in on open and if you're all in on open source and open partners are super important the partner ecosystem around those open source project the companies around it are super important so i want to thank uh all of our partners and all of our sponsors uh please check them out in the expo hall uh we have you can see here uh we have the all the hyperscalers we have all the global si partnering here but also lots of isvs that are present so check them out okay um so we have awesome lineup of speakers uh so we're going to see talks from duck db creator uh we're going to hear from professor fe lee uh we're going to hear from the one and only jensen hang uh professor jin will be talking about small language models uh we're going to hear from texas rangers we're going to hear from general motors block we're going to hear from ryan blue the original creator of apache iceberg uh we're going to hear from t from posit or company formerly known as our studio and then we have lots of lots of announcements from datab speakers okay so in my keynote here i just want to give you guys the vision of what we think uh we can do with the databas platform i'm not going to have that many announcements and then afterwards we're going to hear talks from brickers will get on stage here and most of the announcements will be there okay so um we've said this for a long time every company now on the planet wants to be a data and ai company okay so in the last 18 months every cio even every ceo i meet of a fortune 500 company or a small company think that data and ai is going to be super strategic for them over the next 5 years they think that that's how they're going to win okay that's going to be the main differentiating factor is how they leverage dat nai whether in financial sector or they're in retail or they're in media or they're in health care or in the public sector doesn't matter all of it it's going to be data and ai okay and you know at dat bricks this is why our mission since day one has been to democratize their nii even when we were researchers back at uc berkeley we wanted to take this kind of technology and help everyone in the world to use it back then 10 15 years ago uh if you were an organization you would have to hire 10,000 engineers and build a full nai stack inhouse that's what uber was doing that's what twitter was doing that's what airbnb was doing uh but these days you can actually just leverage these platforms and you know drive business value for your organization but in the last 18 months there's been a lot of pressure can you deliver actual use cases can you actually get into production can we really make sure that data and ai has value and i've been talking to lots of leaders lots of practitioners and i keep hearing the same thing again and again and again okay there's three problems that come up when i talk to folks okay so what are those three problems everybody wants gen okay it's like top of mind for everyone it's coming from the top it's coming from the board uh there's a food fight inside organizations who owns geni so that's number one number two everybody's worried about security and privacy of their data and of their gen okay and their whole data state actually they are worried about security and privacy for the whole data state and that data state today is super fragment m mented okay so you know it has a lot of issues so let's double click on each of these three and go through what these problems are and then i'm going to tell you about the database platform and what we think is the path forward for solving these three okay so let's start with the first one everybody wants gen ai everybody wants ai in their organization they want it yesterday and it's been an amazing year actually you can see here this is a benchmark this benchmark it's probably the most popular benchmark it's called mlu massive multil language understanding it's got like 72 categories of different things testing you on biology history you name it and you can see that the ai models the large language models are getting better and better see it's almost saturating up close towards 100 score and you can't go higher than that uh but it's also been an awesome year for open source so we're seeing that the open source models are catching up okay uh you can see dbrx there by Databricks which was the best model in the world for two whole weeks and uh thank you mark zuckerberg for releasing that model two weeks later uh and then llama 3 but about llama 3 we haven't even seen the largest model yet being released okay so it's safe to say open source catching up very very fast okay so this is awesome uh these benchmarks are awesome the results are breathtaking but actually the problem is when i talk to organizations this is you know actual quote i don't care about standard benchmarks you know i want the model to do well on my data and my use case at my organization i don't care you know if it's doing well on mml or not that doesn't help my company okay that doesn't help me uh succeed with my use case so this is the number one thing okay people don't know how these models are doing and for their use cases and that's all they care about the other thing is we did the survey of our customers and 85% of the use cases have not yet made it into production the gen use cases okay so they're still sort of trying them out they're trying to make sure that they're ready to be shipped but they're not quite in production to summarize the ai problem is how do you do gen on your data in your organization and how do you get it into production okay and how do you do that while making sure that it has really high quality so it's really doing the tasks that you have really well two how do you make sure that it's doing that at a good cost right we don't want the cost to be prohibitive uh and three how do we make sure that we can ensure privacy on that ai so this slide summarizes what we're seeing in the gen space the problems that people are struggling with okay when it comes to privacy that's a whole another concern okay so people are super worried about security and privacy of their ai we're seeing intense pressure there's talks of ai regulation there's talks of even you know maybe they're going to ban open source models people are worried about data privacy uh but also not just ai the data is under attack there's cyber attacks coming in into data platforms um you know people are trying to break into their companies so people really want to make sure that they are secure okay and it's not just security of ai it's all of the data state it's all of the raw data it's all the structured data unstructured data the ai models notebooks dashboards anything that's in your organization this is kind of slowing things down and people are super cautious and super nervous okay they want to make sure that their data state is secure okay and then third the data state is super fragmented this is the number one thing i hear like every call i get on people talk about the fragmentation of the data state they basically say typically if i talk to a cio almost every call i have sounds like this we have so many different pieces of software i don't even know what they do we have one of each you know it's you know i don't even know but we have to cut it down i'm under budget pressure in fact i don't just it doesn't even look like this i have many of each okay i have many data warehouses i have you know many data science platforms i have data silo everywhere and consequence of this is uh lots of complexity huge costs and then lock into these proprietary different systems each of those systems is a little silo that you lock yourself into okay so these are the three problems that we really as a company are focused on trying to address at least we're trying to somehow help move uh things forward when it comes to these three so let me walk you through how we do that so datab we call this the data intelligence platform and our vision uh starts with what we call the lake house so let me walk you through that the this idea we had this about 5 years years ago which is and we actually announced it at this conference which is stop giving your data to vendors okay they'll just lock you in stop giving you data to any vendors okay doesn't matter if it's a proprietary data warehouse in the cloud or if it's you know snowlake or if it's even Databricks don't give it to us either don't give your data to us don't give don't trust vendors don't give your data they'll lock it in they'll raise the price after a few years and you have another silo on that picture now that you have to deal with and you just keep adding these silos over the years okay so what should you do instead you should instead own your own data uh you should have your own data you should store it and cheap hard drives in the cloud called data lakes and just store them there pay for it independently make sure it has separated computer and storage completely from the compute so it's just a basic data lake right like s3 adls um you know um gcs and but we do need to store it in a format that's standard so think of it as usb so that's why we announced open source delta lake project here i think a bunch of years ago maybe half a decade ago uh and the idea is okay so then we have this usb format and once we have this usb format anyone can just plug in their data platform any of those vendors that i said don't give your data to they should just plug in their usb stick into that data that you have in the cloud and then let the best engine win let's see who's best right uh maybe this week it's us maybe next week it's someone else this brings disruption and removes this lock in and also reduces the cost and also lets you get many more use cases because you can use different engines for different purposes if you want so this was our vision unfortunately what happened is we almost succeeded and people are bought in okay everybody here in the crowd wants this they're like we want this uh we want to own our data we want in a standardized format but unfortunately there's this like fragmentation so there's now two camps okay on Databricks we have delta lake uh you know we seeing actually 92% of all of our data go to delta that's about uh four exabytes of data every day so 4,000 petabytes every day that's processed going to delta uh but there's lots of other vendors that are using this apache iceberg format which is another format okay so last week we announced that we're acquiring taber which is thank you it's a week old news you know uh and um we announced that we're acquiring taber taber was founded by the original creators of the apache iceberg project okay so ryan blue and dan weeks uh started the apache iceberg project and we were working at netflix and we acquired this company so i want to tell you a little bit about why did we do that what's our thinking around that okay so the reason we did this is that we want this problem to go away so that you don't have to pick which of the two silos do i have to which of the usb formats do i have to store this in if i store it in this usb format you know these cables won't be able to plug in if if i store it in that one the other engines can't plug in we don't want it to be that way okay whatever you store it in all the cables should just work okay we just want very simple uh standard for everything so uh our strategy is a year ago here we announced project uniform project uniform is part of delta and we're actually announcing it being ga uh this week here and uniform it translates to both of these formats delta and iceberg uh it's already doing that today and it's ga but you know we really understand del delta really well we don't understand all the intricacies of the iceberg format but the original creators of apache iceberg they do so now at Databricks we have the employees uh from both of these projects from delta and iceberg uh so we really really want to double down and making sure that uniform has full 100% compatibility and interoperability for both of those so if you put your data in uniform today it should be a no-brainer it should just work then in the background what we want to do is we want to really work with these communities the delta lake community and the apache iceberg community these are open source communities right with people around you know all around the world they're governed by these uh foundations like the paty software foundation and the linux foundation we want to work with them and actually change the formats and bring them closer and closer to each other so that the differences between them do not matter okay so if you store your data in uniform right now then over next you know period of time as the formats get closer and closer um it won't even matter which one you have like the distinction will go away and i hope that in a year or two we won't even care here about which one it won't we won't it would be like vhs and betamax you know who cares okay so that's our strategy with uh with respect to the data format so we hope that we have just a usb format it shouldn't matter all the formats should be supported uh and it should just work okay so that's problem number one remember the fragmentation your data is locked in now the data is just sitting in a lake in a usb format any engine should be able to access it okay remember the second problem governance security how do i make sure that this stuff is super secure so when it comes to that uh we announced here unity catalog a few years back and unity catalog is probably the most important uh development at Databricks since we started the company it's probably the main reason people actually come and use Databricks these days it's because unity catalog lets you do governance not just for your tables not just you know for your unstructured and raw files but for all of your data state including machine learning models ai models you name it and it's not just access control and security it's also discovery it's also lineage it's also being able to do auditing and data quality monitoring or ai model quality monitoring okay so it's super super important um so uh that's what that looks like so now we have we have delta and then we have for governance uh uni catalog and i'm super excited to announce that we're also open sourcing unity catalog this week here so please go to mat's talk tomorrow on unity catalog and he's going to cover all the interesting things that go into it and uh and we'll see him open source the project um so that's the vision of the lake housee okay so basically all the silos that you had before they can just access one copy of the data that's in a standardized usb format under your ownership okay in a lake like gcs or s3 or adls and uh it goes through one governance layer that's just standardized that's unity catalog for all of your data state okay and that's also open and it also that unity catalog open sourcing one thing that's really important about it is that it supports uh two apis that already are standard for governance hive metast store api and uh iceberg rest catalog api so that's already supported in unity catalog uh in fact i think tab and dat are the only ones that support that uh rest api catalog so uni catalog will also just be building on apis that are already prevalent and everybody's using so we basically standardize the data layer and the security layer so that and you own your data and everything goes through these open interfaces and i think that's going to be awesome for the community for everybody in here because we're just going to have way more use cases we're going to be able to do much more innovation and we'll just expand this market for everybody involved it's just going to be great okay so that's the like house uh but i said data intelligence uh so what is that so what we really excited about as a company moving forward is when you take this sl house uh that supports all of your data and all of your governance and you combine it with generative ai uh in particular last year on stage here we announced the acquisition of mosaic ai and when you combine mosaic ai which was a platform to tr train custom ais on your data when you combine mosaic ai with the lakehouse platform uh you get what we call data intellig okay uh so what is data intelligence data intelligence means that our platform trains generative ai models on your data in isolation for each customer and leverages that throughout the platform for everything it does okay so what does that mean that sounds uh like a m what so what do we want to do what is data intelligence well data intelligence for us is really we want to apply to two things one we want to democratize data and second we want to democratize ai okay what is democratized data what is democratized ai s very similar uh dem marketizing data means that anyone in your organization should be able to access the data directly okay today that's not true your ceo is not going to go access the data and ask questions from the data he or she will go to uh uh you know to the data team and ask them hey can you get me this report and they'll say by when do you need it and then they're going to work on it because your ceo does not speak sql or python or at least doesn't know where to find the data and submit their own queries so we're really hoping that we can democratize this so that if you speak english or any other natural language you should just be able to ask your question from the data and many many more people in the organization should be able to get uh insights from that data so we're very excited about that dem marketizing ai is different dem marketizing ai means uh practitioners like yourselves here in the room should be able to really easily create ai models that understand your data in your organization that's what democratizing ai and we want to do both of these two things okay so let's start with the first one um how do we actually make you talk to your data let anyone talk to their data okay so this is what data intelligence is we want you to be able to ask how's the business doing on its fy goals and we want the platform to be able to understand what that means fy stands for fiscal year in the particular company you're at fiscal year might start 1 of february or maybe 1 of july it should know that and business in your particular company means certain kpis that are most important in your organization the definition of those it should understand those and then we wanted to give you back authorative answers that are certified and that are correct and they don't have any hallucinations and we can actually verify that these are correct that's what data intelligence is for us okay so that's what we want to do that's our vision that's what the whole company is working on thank you that's just a fake screenshot it's not an actual product okay but we have actual real demos live but you know uh i'm not going to take the risk anymore uh okay uh too nervous okay uh and then um this is you know if you click on a random table data set in your organization chances are you get something like this that's you know completely hard to understand what does this even mean okay tbh i n c lcore sore key this is why your ceo is not asking questions from your data like who knows what this means well with data intelligence actually already today we can fill out so in unity catalog we already fill out in english text what all your data sets do using generative ai okay so we fill it out like this it describes exactly in english what all the data assets are doing and when we have that then we can actually do proper search so when someone comes in and says where do i find shipping information for homegoods it knows that okay you're looking for this particular data set over there because it has all those descriptions okay so that's data intelligence that's what we working on to make happen and then i do want to be put a big plug for something that's already in databas today which is the assistant that we've trained uh using dbrx we' fine-tuned it on actual all of Databricks documentation all the errors that we've seen since we started Databricks and you can ask it today to do pretty advanced stuff you can say hey write code for me that you know does the streaming thing you know write your code like this okay it just works i use this every day okay so this this has made me much more productive i'm getting a little bit rusty with my coding uh but just work with the assistant and it gets things right i can even like write half pseudo you know kind of looking right code uh but you know no compiler would ever accept it um but the system will just fix it like this there's just uh fix things for me this is used with over 100 thousands of users every day use it in Databricks platform so i do think this already is democratizing access to data and more and more people are able to talk to the data ask questions from it okay so that's what um data intelligence is when it comes to talking to your data okay what about the marketizing ai uh when it comes to marketizing ai that's where our whole ai generative ai stack comes in and we're going to hear in you know two talks you're going to hear the details of this and all the announcements that we have around this but i'll just give you the high level uh overview in mosaic ai we basically have all the serverless gpus around the world and we just enable you to very eas easily seamlessly in a ui be able to build your own ai on your custom data and productionize it and evaluate it okay so you the first step is preparing your data so that this ai ready so the platform is great for that second step is how do you train or how do you fine-tune whether you want to use some fancy techniques like laura or you want to continuously pre-train the whole model or you know you want to use a vector search or whatever you want to do uh you can build your ai there then you can deploy it in production deploying in production means you're running it on r gpus um in you know different countries just in a serverless manner they're just ready to go we're paying a lot for those gpus today uh my cfo reminds me every week um and uh um and that's that's where you can run these you can run our vector search database there uh and then we have evaluation we're going to talk a lot about evaluation here how do you actually know your ai is doing well in production and then of course finally really important for most people in this room how do you govern it so how do we actually secure it how do you make sure that we can track it rate limit it track the tokens um you know make sure it's not doing something that we don't want it to do so that's the mosaic ai okay so uh we'll hear from my co-founder patrick wendell uh who will talk about this in detail okay so that's the that's the full platform of Databricks um so that's what we get the data intelligence platform you have unit catalog you have delta you have all these things accessing it okay one thing one last thing i want to announce uh here is that we're also super excited to announce that all of datab breaks now is available in serverless okay okay so 1 of july you'll be able to get everything in data rs okay so whether it's our notebooks or whether it's our spark clusters or whether it's you know workflows job processing all the different aspects datab so far only a few parts of it was serverless now you get all of it in a serverless fashion this is a project that has involved hundreds of hundreds of engineers for over two years it's been a long project internally two three years ago uh my co-founders uh mat and i uh told the company we got to build a lift and shift simple version of serverless and actually our engineers pushed back and said hey you guys are wrong we should redesign it from scratch for the serus era and we told them nope we decide you know in the company and uh turned out we were wrong and the tech leads were right and they've been working really hard for 2 years so we basically redesigned many of the products you know the notebooks the jobs everything as if we had started a new company uh what what would that look like in the serverless era okay uh so how do we make sure that it instantaneously comes up there's no more clusters everything just works super super fast um under the hood uh we make sure that we multiplex the resources in fact today uh you're paying us for idle time if if you're not using serverless actually you're paying the cloud vendors a lot of money uh and then you're paying us in addition to that uh for idle time uh with serverless um you know you're just paying for what you're using there in fact there is no cluster to set up for it to be idle or not be idle okay so we'll take care of all that for you under the hood and one thing that we're excited about this since we own all the machines now it's no longer this uh you know joint responsibility over machines that are running in your account and in our account we were able to really redesign so that this year we'll be rolling a disaster cover in a different way that's really custom for server loss uh cost control so you can really do the tracking and you can do the tagging and you can really use ai to predict where your costs are going on the serverless infrastructure and we were also able to do security a different way because again we own all the machines so we're able to really lock it down in a different way that's not possible when uh when it's not serverless so we're very excited about this you know all these knobs that we had before are gone right um cluster tuning you have people setting up clusters what type of machines should they use spot instances this and that should it auto scale that's none of that is available anymore it's just gone there's no such page you can't do that the data layout how are you going to set out exactly your data sets how you going to optimize your data sets that's also gone okay we're just optimizing it behind the scenes because it's serverless we'll just run in the background optimization on your data sets to make it really fast and optimal using machine learning uh so that's also really awesome capacity planning um usage tracking and then my favorite thing is no more versions okay so there'll be no spark versions uh when you're using this this uh serverless edition you can't pick a version of spark anymore so you don't have to worry about thebr upgrades and those kind of things so we are super excited about this please start start using serverless and in the future new products that we roll out like next year when i'm here on stage they'll probably only be available in serverless okay so if your organization is not on serverless please get on it we're rolling out serverless infrastructure all around the world uh to make sure that we have availability in any region that's near you okay whichever country you're listening in from right now or representing where you flew in from we hopefully have a serverless infrastructure near you there okay all right so that is the data intelligence platform okay and today we have over 12,000 customers that are actually leveraging this platform but enough hearing from me uh i want you to welcome on stage brian from general motors who's going to tell us how they leverage the data intelligence platform uh to build better cars [applause] i got hey Databricks community good to be here with you and Databricks thanks for giving us a few minutes to share our story the gm mission zero crashes zero missions zero congestion to some people words on a slide to others like me deeply personal you see i know a six-year-old that was killed in a car crash so this future cannot come fast enough for me but is it strategic yes customers increasingly want to vehicle that thinks about their safety security and comfort so the company that figures these things out will increasingly write the rule book of the future and gm wants to be that firm but there's a trick to get there this 115 year old company that is his stored legacy and hardware must become a software company no easy trick so where do we start gm has a ton of data that's not the problem we had a beautiful on-prem infrastructure why change well two reasons number one was data efficiency gm did a time in motion study and we determined that every year we spend 200 person years in the hunting and gathering of data now don't quote me on that we've made a lot of progress since that study was run but you get the order of magnitude of the problem but more importantly in the last few years the world changed and gm understood that if we didn't have ai and ml in our ar we could find ourselves at a competitive disadvantage so we needed to transform and about 15 months ago gm decided if we're going to change the future we must change ourselves today so how do we go about doing it well we were going to be all about the cloud which meant that we needed to shift our culture safety first safety first safety first that was immutable rule but underneath that it was all builder mindset we were going to get in there we were going to build things and we were going to learn and grow and that was going to be how we were going to approach it we would find patterns that worked in the cloud and we would build an insight factory that would be the best in the west coast approach the best of the cloud and we print that out into a blueprint that we would share with others at gm and to do that we decided we would sorry i'm catching up build it all on Databricks why because we had totally aligned interests we needed to move from data and solution silos to single sources of truth with rapid collaboration we needed to move away from fragmented governance into simple unified governance and we felt if we did those two things extremely well that we'd be able to go from pockets of limited ai and ml execution and gm has some wonderful things to really building ai ml into the dna of gm if we could do that maybe we would change gm forever so this is what the insight factory looks like today i'm super proud of the work of the team it's an endtoend system with a react ui that allows us to control all of the data we define the quality and the medallion process so we can see every hop and no hop missed we control the strategic ip that goes in there because we should own it that's our competitive advantage we can refine quickly and present insights in a beautiful gm branded react interface but i have to admit building this isn't easy a system like this super complex it's dynamic everything's changing every day and on top of that you make mistakes so we pushed ourselves to our limit and sometimes beyond but we built it off of scar tissue and grit and it took a village but the good news is we got here in a record time so imagine going from almost nowhere on this to having this entire system in as like maybe nine months here some good news for us to share a unity catalog basically worked out of the box for us got us in the game and the open ecosystem allowed us to have a low friction way to go after that hunt and gather stat i talked about before using amuta for policy compression and atlin for end to-end visibility from the cloud all the way back to our on prem two lessons learned along the way managing a node in the mesh takes a lot of technical talent so when things like serverless and lakehouse apps come along it really helps us with our ability to scale Databricks community please keep those coming they're really being put to good use but the other thing that we learned is gm has a lot of very smart and talented people already they've built ai and ml but it's in these little pockets and in these silos so something like ml flow that gives them a path to production you can feel gm's data intelligence rising already on top of that everything that's happening with gen we're going to lay her in next year and we feel like with that we can go toe-to-toe with anybody in the world so we opened the factory and we just needed the killer app so about 2 weeks after the factory open we got a knock on the door from a guardian an angel i don't think that's her actual title at the firm but one of the nicest people in the world and her job is customer safety and at general motors that's a big deal nothing is more important and her job is to know the health of all the cars on the road which ones might need a little bit of attention or service she was experiencing this exact problem insights that she should get in an afternoon might be taking her days or weeks or even a month she was having to go to all these silos and do complex joints now imagine her world tens of millions of cars on the road different combinations of sensors and all these new cars are coming in and they're more intelligent more sensors so this is near a max complexity problem and there's only one way out of it and it's going to be ai and ml so of course we could help we welcomed her into the factory 15 underlying sources we plug that into the cloud rapidly expose the data in unity catalog and gm was scratching their heads the meeting that we went into where we said we brought this all to life in the cloud they were like i thought we were going to have a follow-up powerpoint meeting or a discussion to align on the next steps we said no we're just building we started to hammer out the first silver table and it was terrible nobody liked it then we had another meeting we said listen we're going to pivot and rapidly get this better and they said this one has too many warts but then gradually because we all had the same mission in mind we were seeing the same data we started to come into alignment and that's the way gm's going to get to this future smart people looking at the same data and getting into flow state rapidly is the path out of this so to me that's a great start it's year one at gm we got momentum the company got oriented we put thousands of people into Databricks we're reducing the time to insight and we're finding ways to contribute value in year two we're going to lay around ai and ps perhaps take another step at gm towards our mission of zero crashes so if you're a person that loves a good challenge we have the mission and the data and now the tech what we need are people that are will willing to change the world please take a look we'd love for you to join us at gm thank you enjoy the rest of your [applause] [music] conference all right isn't that awesome um i think every year that goes we're going to get closer to this zero emissions zero crashes i i would love zero congestion i mean also the others but congestion is is my you know least favorite one um okay awesome so i mentioned to you that uh we're hearing from customers all the time they want to build ai on their custom data and they want to do that while having great cost and great privacy okay that's what we call data intelligence custom ai on your data so i want to welcome on stage my co-founder patrick wendell that are going to tell you all about mosaic ai welcome patrick [music] wow there's a lot of people here hey everyone hope everyone's doing well uh yeah thanks it's great to be here and talk about the latest in generative ai things we're working on at Databricks things our customers are doing as well as the latest in industry research uh so i'm going to start with a slide from ali's talk it's been a super exciting 12 months because as of you know 12 months ago we had one really great frontier model that was a super highquality ai model and today we have five or six amazing frontier models several of which are open source so to understand how we leverage these capabilities it's important to know how frontier models actually work so the way that these large scale gener of ai models work is that they're trained on data from the internet and in fact the big breakthrough with a gpt3 was that the scale of training data went way higher than had previously been didn't done and it turned out that resulted in a much better quality model so these models are trained on internet data and then they're optimized and evaluated on how well they do on what are called general knowledge tasks so this the last slide i had had a um a benchmark that benchmark is mlu it's kind of the canonical benchmark for how these really frontier ai models are evaluated and uh not a lot of people look inside mlu but it has 50 different categories in which they're evaluated on sort of general knowledge so i like to think of it like if you're jeopardy it's kind of facts and information about many different categories so i just pulled this up here's a few example categories that are in the mlu benchmark uh some are expected some are kind of surprising so nutrition world religions uh astronomy human aging is kind of a funny one you wouldn't expect i actually played with chat gbt this morning uh just to test this i started asking questions about wrinkle reduction it happen to be like have encyclopedic knowledge of uh how to reduce wrinkles as you age so you know they're all chasing these benchmarks trying to get good at these particular topics um but what we work on at Databricks is not so much general purpose ai models but it's actually helping our customers build ai capabilities into their products and their services and those capabilities need to have a very deep knowledge of the context of that product and the data that powers that company and product so what we're focused on is not so much these general purpose knowledge tasks but it's how well your customers are able to benefit from your ais so for example if you have a customer support ai you know how effective is that in answering customer questions most people don't ask their customer support about wrinkle reduction they ask about you know solving the problem that you have with your product um you know if you're generating code in your in your ui how often are you generating good code that your users are accepting and if you're creating marketing content with ai you know is that marketing content on brand and does it match your company and the way we like to think about this is that what we focus on at Databricks is not so much pushing the frontier of general intelligence which is an exciting task in and of itself but it's really the data intelligent application so helping you build data intelligence into your products and services and today i'm going to share some of our findings working to build data intelligence systems with thousands of different customers so i'll give examples of types of systems that are being built as well as the fundamental technologies that we're building to to help those come to production and this is actually not just an active area of interest in industry it's a very active research areas how you take these general purpose capabilities and you adapt them to specific scenarios and specific tasks and the uh the sort of leading research in this area points to a solution called compound ai systems this is a paper out of berkeley uh that's just one of many different uh research groups that are looking at this problem and what compound ai systems do is they take the general capabilities from leading ai models but they customize it substantially they do things like tuning models adding retriev and search to your to your model giving your model the ability to use tools and take actions in your enterprise and it's through these compound systems that our customers are able to build really really high quality embedded ais in their applications uh so this been a little bit abstract so i can give an example here uh so fax set is a datab brak customer and uh for those who don't know fax set is a sort of leading uh a company around financial analysis and uh builds products and services for basically people that are in the financial markets and this is a a picture of the fact set ui uh and though people can use the point and click ui in fact set what most of their power users uh use to get you know all the information that they have on equities bonds and so forth is they use a query language called fact set query language or fql uh so this is an fql statement uh in this box i don't expect you to understand what it does i'll get there in a minute but the opportunity in with gen for fax set is that instead of having people type these this sort of very specific query language they need to go learn it would be a lot nicer if someone could just say in english or in their preferred language what they're trying to do and the fact set suff could just do it for them so here's an english version of what that query is showing uh give me the current year and trailing earnings per share for all us listed equities so even i understand that and i'm not a fax set expert the the opportunity here is that you know fax set could be approachable to many more users and existing users could be much more efficient if they can just use language to say what they're trying to do so how did faet build this well they actually started by taking one of these general purpose frontier ai models and trying to just give it a few examples of their query language but simply just call into this existing model and have it translate from english to the desired query and unfortunately this didn't work very well the the accuracy of this technique was only about you know 50/50 so it would like half the time generate the wrong answer and it was also extremely slow because of the amount of context they needed to give the model was like 15 seconds so that was not really sufficient to build into the fact that a product so how did they solve that well they built a compound ai system and i won't go into a ton of detail but that system involved tuning some open source models fax that happens to have a huge amount of data of existing queries with labeled english examples so they could tune a model that understands their data extremely well they also included search and retrieve so they could go and search and look up things like if i you know mention a company's name but i don't mention the ticker symbol you know that could go search in a database and be able to resolve that and they also tuned and customized other parts of the process and through this they were able to get 85% accuracy and triple the the performance in terms of speed so this kind of met the bar for what they were able to put inside of their product so fact set is just one example they're actually giving a talk i think at the conference so if you want to learn more you can go to their talk but at Databricks we're focused on building general capabilities through our mosaic ai platform that lets any company do this type of customization to go from generally intelligent models to data intelligent products and services this is actually uh the result of an acquisition we did uh that many of you may have heard about of mosaic ml and we're happy to announce today that those the results of acquisition are fully integrated now into Databricks product and services and i'm going to walk through some of the individual capabilities we're offering in this in this area so as ali mentioned it's a life cycle from preparing your data customizing models and deploying applications in production and the first uh and the first area of those three uh we're happy to announce today a zero code fine-tuning of open source models in Databricks so what that lets you do is start with a really high quality existing model that was already trained and then with no code tune that model on your enterprise data to be really good at the particular task that you care about Databricks will manage all of the optimization you know tuning is actually fairly complex there's different types of parameters you can tweak there's different ways you can do it Databricks will fully manage that but you end up with a fine-tune model that you own and you can use that model in your ai product or service uh there's actually several companies talking about their use of tuning on Databricks at this conference so i won't uh uh spill the beans on these talks but uh a few that you might check out uh one is fox sports by the way i think fox sports gets the award for the coolest animation in theirs i'm a b football they have football animations throughout the whole so even just for that i think it's going um but they're customizing ai using lots of data they have they have a hundred years of transcripts from nfl and other leagues of people uh um discussing ongoing sporting events they can use that to customize a model and to be able to generate live commentary and things like that at lan it's very similar they have a ton of existing uh they're using ai in their product to generate code and generate um dashboards and so forth and they have tons of labeled examples of doing that so they've customized and tuned models for that purpose so tuning takes an existing off-the-shelf model and kind of tweaks it to be better at the thing that you care about for your product or service but in certain cases companies have so much data that it's in their interest to actually fully build a model from scratch so this is called pre-training in the sort of ml um language but what it really means is that you create a model from your entirely from your data that didn't tou the internet and didn't touch any other kind of data so a great example of a customer doing that on Databricks through our mosaic ai training platform is shutterstock and shutterstock is actually announcing today at this conference a brand new state-of-the-art image model that they're able to expose and let their customers use a little bit about what shutterstock does they're actually the world's leading or one of the largest databases of proprietary images and they can take advantage of of that huge data set and ip that they've accumulated over the years and let their customer generate totally custom images for marketing purposes or personalization and unlike other image models that are trained on internet data uh the shutterstock model is completely trained on a trusted data set that they have full rights to so they're able to take this intellectual property they have and build an awesome model in order to share that with their customers and they'll be talking more about that model at this conference so we're actually excited to share today that more than 200,000 custom ai models have been built on Databricks for use in enterprise ai systems and boy just the hardware required to do this is is insane between gpus and other types of ai accelerated chips so our friend in the leather jacket that's going to be talking later we owe him a lot of thanks uh for for creating the ability for us to do this so building the the underlying models is uh one of the most important part of ai systems but the next part is extending the model with capabilities uh beyond just basic data reasoning by far the most popular way to extend models these days is something called retrieval augmented generation that's really the ai community likes to use fancy words that really just means your model knows how to search and what matters for enterprises is that you can have your model search over proprietary and custom data sets that you have uh Databricks released earlier this year the ability to host uh to to manage your data in a hosted search index a vector database and we're super excited to announce today this month that that offering has gone ga and we've also added much uh state-of-the-art embedding model in that offering as well a little bit of an example of how that's used uh corning is a a materials research company and what they're doing with ai is they're building an ai for their c for their internal research team that's their sort of core ip gener to be way more efficient in exploring different types of materials to research for industrial use cases and they use our vector search engine to include tons of proprietary information on patents and prior materials research that only corine has that's not available in general purpose ai models to make that app work really well for the researchers so retrieval is a really popular technique this search augmentation but i do want to emphasize that it's just one type of tool of many and what we're increasingly seeing at Databricks is our customers desire not just to do search in order to do question answering but to actually have tools and abilities that can manipulate and take actions based on the person who's using the ai so a few examples of types of tools our customers have built you may want to extend your ai to open or close support tickets if it's a support ai you might want your ai to be able to execute a small amount of code on behalf of a customer and execute it in a secure environment these are called tools and very similar to base models tools need to be heavily customized and written in a specific way for each enterprise use case you know a general purpose model is not going to know how to interact with your business system your ticketing systems etc so today we're actually announcing the mosaic ai tool catalog in Databricks so what this lets you do is it lets it lets engineers and scientists in your teams author tools they can build those tools on top of secure compute abstractions and then they can publish those tools for use by other people building a applications inside of your company so this can separate the usage of tools from the authoring the authoring may involve credentials or other sensitive information and it lets all of your engineers discover these tools and then use them now the real benefit of tools is not just the individual use of a tool but it's actually combining the tools to create what are called agents another fancy term an agent just really means an end to end ai application that can do something on behalf of your customers or users so a support support bot would be an example of an agent and we're also happy to announce today a framework for authoring deploying and evaluating agents inside of Databricks called the mosaic agent framework this framework can work with your existing lang chain or other chaining frameworks but lets you author your your agent in Databricks deploy it to an api endpoint and test its quality so my colleague casey will actually demo soon a lot of these abilities around tools and agents that were super excited to ship so the last piece of the puzzle is how you evaluate and understand quality uh ai is a super exciting space right now because it's so easy to quickly demo and put some data in an ai and build something that seems to be pretty cool but what's really important when you go from a demo to a deployed application is that you make sure the quality is really really good i'm actually kind of curious to ask show of hands how many people in the audience have done some kind of internal they've either built or used an internal demo of ai on the data inside of their company how many people have done some kind of demo like that curious okay so it's like you know 60% of people so we're seeing that everywhere every company is building internal demos but then they need to go from the phase of having a demo to a deployed application and what's really really important is that you make sure the quality of the generated content is good we talked at the beginning of the talk that you know the whole difference between general purpose models and deployed ai applications is that you're optimizing for something that's not general knowledge you're optimizing for closing tickets for you know generating high quality code for helping your customers but it's important that you systematically measure that so you can make sure you're doing a good job at it and you're improving it so the third piece of this agent framework that we built is an evaluation tool that helps you employ state-of-the-art techniques to measure the quality of the thing you're building so the way it works is you can first give a few examples of high quality interactions then as you iterate and tweak your application as you tune models as you experiment with search or integrate tools you can check is the quality of this thing improving or is it getting worse and what's more important is that once you're at scale once you have users many users using your application the bottleneck becomes how you get lots of really really high quality evaluation data so you may have hundreds thousands of interactions and you want to understand are these high quality are they working well so our quality our agent evaluation also lets you invite experts that could be part part of your company or they could be external contractors to evaluate and score actual ai interactions and finally it lets Databricks learn how to train ais to do the quality scoring which can allow you to scale this up and actually evaluate almost every interaction that you're having inside of your application ml flow 2.14 which comes out on monday the team promised me they're shipping it on monday so i'm holding them to it by talking about in the keynote um they will also include a substantial quality tools around these gener of ai applications in particular the ability to log and investigate traces when you have lowquality interactions so that's that's often a big piece of the quality puzzle is if you had a user that gave thumbs down the thing didn't work so well what exactly went wrong was it an issue with the retrieval was it an issue with the way i tune my model and so that'll be part of ml flow and if you're using it in Databricks will also deeply integrated into the Databricks ui cool so we talked about building customized ai systems deploying them and understanding their quality the last piece of the puzzle is governance and it's a really exciting time in in ai everyone wants to move super fast they want to compete in their markets they want to build ai based applications but particularly for enterprises it's very very important that the ai that you deploy is safe and it's trusted and an issue we've seen in a lot of our customers is is sort of they're victims of their own success in some way way you have one ai project that's really successful and then you turn around and now you have 10 and now you have 20 and now you have 50 but how do you as a sort of a a global company make sure that they're all adhering to the safety properties meeting the quality that you expect and so forth so a a good example of a customer that uh that had this type of challenge on Databricks was edmonds um so they had tons of ai a projects going to production but things started to become a bit of a sprawl so you had different teams that were managing their own credentials to some of these third party model providers the cost started getting uh um really out of control and the capacity management was also an issue you know gpus are a scarce resource you want to make sure that your most important applications have access to the most important uh the the the resources that you have available to you for serving so we're also excited to announce today the mosaic ai gateway which is a central point inside of Databricks where you can can uh you can enforce all the sort of um uh auditability and governance requirements that you have around use of your models so this isn't meant to slow teams down it's actually uh from our experience helped individual engineering teams move much faster because they have access to a very specific you know set of approved base models they have a set of guard rails that's been agreed on for the company that they can use and they can actually innovate faster and not sort of each reinvent the wheel around these foundational capabilities [applause] awesome so as i said we've been working on technologies to help you know not only the customers i mentioned here but our 10,000 customers build and deploy ai products and systems and i'm super you know i always believe in show not tell so i'm super excited to welcome my colleague casey ulan who will actually be taking you through these in a live demo she's doing it live so you know caveat in case there's any issues she's going to take you through a live demo of these capabilities in our product and how an example customer might use them so please join me and welcoming casey y thank [music] you all right thank you patrick so all right where are we all right so i work for a cookie conger rate that has a bunch of franchises and i want to create an ai agent to help all my franchise owners improve their business by allowing them to analyze customer data allow them create marketing campaigns and and analyze and develop sales strategies and so one of the things that they'll be able to build with this ai agent is an instagram ad campaign where they can promote the bestselling cookie in their franchise and the ai agent is going to create an image for the instagram app as well as a caption that's going to capture the hearts and minds of all of our cookie lovers to really drive sales so i gave general intelligence to my franchise owners with the off-the-shelf model and it was giving good results but they were too generic and they weren't tailored to our business or the individual franchises and this is where the mosaic ai platform comes in mosaic ai is going to allow us to extend this general intelligence with our enterprise data so that we can have data intelligence in this demo we're going to build uh sorry in this demo we're going to build uh an agent that's going to use the unity catalog tools that patrick just mentioned so in this architecture we're going to leverage these uc functions that can now be leveraged as tools and uc functions can be sql functions that access your data warehouse they can be python functions they can be mod model endpoints and they can even be remote functions which are going to allow you to call external services like slack or uh email or even file a ticket if you need to so to build all of this we're going to use the mosaic ai platform and with that we're going to go ahead and jump right in so here i am inside uh so there's three capabilities that we're going to use inside of mosaic ai to actually build this data intelligence so the first is we're going to use our tools catalog to actually build the data intelligence then next we're going to build uh and understand our quality with agent evaluation and then we're going to be able to debug and improve our quality with the mlflow tracing capability so with that let's dive into Databricks so here we are inside unity catalog you can see that i have some functions that i'm going to use as tools and these are governed alongside my ai my unstructured data and my structured data so to help demystify what a tool is we're going to go ahead and click into our franchise sales so you can see in here it's just a simple sql query that's accessing my sensitive transaction data and this is where it's really important that your tools are governed alongside your data because only the people who have access to this underlying transactions table are able to successfully call this tool and that's why we need this centralized governance across data ai and tools so some of the other tools that we've created that are leveraging our enterprise data are in here as well so franchise by city and by country are just like helper functions to help me get the sales data and then this franchise reviews tool is actually grabbing customer reviews from our social media site so all of these tools are leveraging my enterprise data so we're going to go ahead now and extend a base model with these tools so i'm going to come over here into the ai playground and we're going to jump in and then from the ai playground i'm going to select a tools enabled base model and so i'm going to you can tell it's tools enabled because it has this little icon on the right so i'm going to go ahead and select llama 3 from here i'm now going to add hosted tools so these are my unity catalog tools that are hosted inside the secure and scalable data rcks environment so in here we're going to access the tools that we just showed you in the ai schema so i can use the syntactic sugar to grab all of those tools and then my marketing team has has created a tool for me so i'm just going to copy paste it real quick because it's kind of long so we're going to put this in here and this tool is going to generate an instagram image using the shutterstock image ai model that patrick just announced as well as a caption so now it's time to actually test this before i forget we're going to crank our temperature down to zero because this is a live demo uh and then now we're going to quickly copy paste a prompt in here so this prompt is going to say hey send marketing an instagram post with an image and a tagline for the bestselling cookie in the san francisco store so we can increase our sales and show that we listened to customer feedback so we're going to go ahead in here uh and click uh oh we're doing it live uh well that should have unfortunately worked hold on we'll try again so we'll go back in here we're g to add our tools ai star and then we're going to come in here and add our marketing oops oh it's now in our auto complete in here uh and now let's try this again h unfortunately it looks like there's a connection with our h unfortun uh what is this catalog ret oh oh oh i'm so silly i typed in the wrong name of the function you all should have caught that for me i need to type in retail prod. thank goodness for error messages so we're going to come in here and now we're going to add this in uh and now we're going to go ahead and do this thank you thank you all right so what is happening here is going to be a little bit magical so we're going to come inside oh hold on h oh my goodness all right we're going to have to bear with me we're going to have to do it all again all right we're coming in here uh coming in here sorry uh this is why you don't do it live all right retail p. a all of the functions in there we're going to add in our marketing tool that's going to generate our instagram ad all right we're in here we're going to send the prompt and we're going to make sure it's temperature zero which is what we kind of forgot there temperature zero live demo all right here we go now we're going to send this and what's happening is going to be kind of magical so as we come in here you're going to see that llama 3 is going to do chain of thought reasoning so it's going to figure out which tools it needs to call in order to execute this so you can come in here and say oh the first thing i had to do was grab the franchise id for my san francisco store the second thing is it needed that to be able to access our franchise sales because it's trying to identify what that bestselling cookie is then it's going to grab all of that sales data in here so it grabbed all my sales data and from here i identify the bestselling cookie is this almond biscotti cookie from there it said hey we need to show that we listen to customer feedback so we need to look into our customer reviews tool from there it's going to ask hey what do customers like about this biscotti cookie it's going to say oh they like the crunchy texture and unique flavor and it's going to send all of this to my slack tool that's going to generate this instagram image and caption and send it to my marketing team on slack so they can review it before we post it on our social media and so now now the moment we've all been waiting for is let's see what it actually returned so i'm going to jump over into my slack and this is the image that was generated by the shutterstock image ai model that shows our image biscotti and then you can also see that it creates this customized uh caption where it says our customers rave about our biscotti for its crunchy texture unique flavor and perfect coffee dipping quality and so this is what has generated and this is how you can use data intelligence to extend your general intelligence to improve a base model now what happens if i remove the intelligence so i can come in here kind of like we did earlier and we're going to remove all of these uh enterprise data enabled tools and we're going to run it all again so now we've taken away all of our enterprise data access from this and now it's still going to generate an image and it's still going to generate a caption according to that prompt but it's going to be much more generic so as we jump back into slack and it's going to show me my new image soon coming down here okay successfully sent here we go so this is the image that it now created so all it had to go off of was that it's a cookie that's in s francisco and so it tried to create some kind of cool instagram ad yes and and if you take a deeper look at the caption it's very generic so it just says our bestselling cookie is backed by popular demand share your favorite cookie moments with us and so not really tailored to our specific business or the franchise or using our enterprise data at all and this is why data intelligence is so important so we just showed how you can use the tools catalog to extend your general intelligence with your enterprise data to create data intelligence but how do i know that this agent is high quality so the way that i know it's high quality is i'm going to use agent evaluation and mlflow tracing tools so agents are really hard like how do you know if what we just did was good or bad so we're act and there's so many different things you can do with an agent as well so we're actually going to have to launch a pilot program with a subset of the franchises and give them the agent evaluation review app so this review app is going to allow all of your franchise owners to interact with your agent whether or not they have a datab breaks account and then it's going to allow them to give feedback on the response resp so they can come down here and they can say yes and they can explain why or why not uh that this answer was good and then they can go ahead and click done and submit this feedback this feedback that they submitted is then logged in a delta table in unity catalog in your account that you can then build an evaluation data set off of get confidence that you can go into production or as i have done i enabled lakehouse monitoring on top so that i can observe how my pilot program is running so you can see over here are the different franchises that i set up this on and here is me tracking their negative scores with the agent over time and you can see that something is going terribly wrong with the los angeles franchise they're getting a lot of negative feedback on the agent so if i scroll a bit further to actually investigate their ratings and what questions they're getting they're rating poorly we can see that their actual feedback that they're giving me on here is that it's returning ir relevant reviews so it's returning reviews from san francisco stores or non-la stores to them or it's even hallucinated that there's a liberty chip cookie which we don't sell one of those at the cookie conglomerate so we're going to need to dive in deeper to figure out what's going wrong with our quality here so we're going to use mlf flow tracing to get deeper and figure out what exactly is going wrong so i'm going to jump inside of a notebook where i've actually queried my assessment logs in here and in here we have captured an mlflow trace automatically for you so ml flow is a popular tracking api for geni and machine learning experimentation and deployment and so we've extended it to now work with compound ai systems where you can now trace your input to the system and how it's transformed as it goes through every step of the system along the way to actually create that output in the end so i can click on one of these traces and it's going to open up this stack view if i click the top of the stack you can see the question that was sent to the system and the output so you can see this is the one that's saying what our customer is saying about liberty chip and then this is the absolute hallucination which we learned from the review app where it's saying hey customers are raving about this cookie and it's out of this world delicious but we know there's no such thing as this cookie so we need to figure out what went wrong so we're going to go over into our stack and we're going to dig deeper into it and so we're going to go to our first tool that was called which is this customer reviews tool as we go in here we're going to see what the input and output of that were and if i go here you can see okay it's saying that the liberty chip cookies out of this world delicious so this is definitely where the problem's coming from so we're going to need to dive even deeper into the stack and when we get down here we're going to get into our retriever so this is the thing that is actually retrieving our customer reviews and so if i look and now because of tracing i can actually see the exact reviews that are returned and so i can see this review is saying hey the staff is warm and welcoming the store was spotless and the cookies were out of this world delicious so what's happen happening is my retriever because i can't find anything about this liberty chip cookie it's just returning random reviews and so i'm going to need to do two things to fix this the first i'm going to need to actually increase my criteria threshold for relevance on my review app retriever so it says like hey don't return reviews if the relevance is below a certain threshold and then i'm going to need to do a little more prompt engineering to make sure that if the context that's given to my model isn't relevant to the question that was answered don't just summarize that context so i've gone ahead and i've already made those two fixes and i've redeployed my agent and so now we redeployed the review app as well sent this out to my franchisees and in my pilot and so now they can say what are customer is saying about the liberty chip cookie and so if i type this in here you can see that now instead of hallucinating it's saying that liberty chip cookie is not mentioned in the reviews it's possible that's it's not sold in these stores which is exactly what we wanted to say when this happens all right so in this demo we just showed how you can use the tools catalog to build data intelligence by extending a general mod or general intelligent model we saw how you can use agent evaluation to actually understand your quality by getting your agent into the hands of your franchise owners even if they don't have Databricks accounts and allowing them to give that humanin the loop feedback with thumbs up thumbs down and then we used mlflow tracing to allow you to debug and iterate on your quality to improve your agent so we talked a lot about many different things in this talk but there's three main key takeaways that we want you all to really gather about the mosaic ai platform the first is that we have to move from general intelligence to data intelligence and the way that we do this is we augment general intelligence with your enterprise data and this is going to give you much better insight into what's happening in your business as well as it's going to improve the quality of your applications and we saw this with the franchise cookie agent where the images and the captions for our instagram ad campaign were much better once we gave it access to that enterprise data the second thing is that you can also improve quality by moving from these monolithic models to modular modularizing them down into ai compound systems where now you can specialize each step in the system to improve your quality like we saw with the fact set used case and also in many cases will also improve your latency and lastly the mosaic ai platform is the best platform to build highquality compound ai systems we have thousands of customers using uh the mosaic ai platform today and one of those customers is block and so i'm very excited to welcome jackie brosser from block to the stage who's going to talk about how her team has leveraged the mosaic ai platform to build and empoy generative ai solutions and so with that welcome jackie brosser to the [applause] stage good job you're a b thank you casey i'm jackie brosser and i'm the head of platform engineering for ai data and analytics at block today i'm going to tell you oops today i'm going to tell you a little bit more about the journey we've had taking ai into real business impact using the Databricks platform unfortunately i don't think my clickers working all right block is a really unique company because we have a bunch of decentralized business units that are all united by this common purpose of economic empowerment we have square which is our first business unit where millions of small businesses leverage our products to take payments and grow their business through add-ons like banking we have cash app where hundreds of millions of different consumers use our products to send payments to their friends and family as well as manage their finances through products like investing and borrow we have tidle which is a music streaming service founded by jay-z that helps create creators to monetize their creations and we have tbd which is a subsidiary working on decentralized technologies like blockchain and web three identity one of the really unique challenges of our data platform team is that we have to have one data platform that's able to support all of these different diverse use cases which also come with different um people culture and practices and so we have to make sure that we have a really flexible platform that can not only scale across these different types of data we have you know billions of payments going through square and cash so we have to really operate in on scale when it comes to structured data you know moving in real time we also have tons of unstructured data from title and so we have to be able to handle both types and it's really been a challenge to have a a platform that's flexible enough to not only cover all of these use cases but also be able to scale to new use cases that we didn't see coming like generative ai before we talk about the platform behind the scenes that allows us to productionize all these use cases let's take a minute to look at one of these use cases in the wild so here we have one of our generative ai use cases for square where we allow a small business to onboard and automatically get a suggested menu um that can get them started right away without having to go through and manually fill in one of the principles we've really tried to rely on as we think about how to use generative ai is this principle of giving time back we want to give time back to our square sellers so that they have a chance to focus on the parts of the business that really differentiate them and we can automate away those non-differentiated business operations like onboarding and creating a menu and so in this case we've seen up to 15% time savings for small businesses to you know get set up and start making money in with small businesses with really small margins that can be a really really huge advantage not only do we focus on generative ai for a lot of these external facing use cases but we also have a really big emphasis on internal facing productivity use cases like code generation and workflow automation and we all power these use cases through this flexible Databricks platform that we're going to talk through in a little bit more detail now we wanted to build our ai platform in a really flexible scalable way where the data was already securely stored in this federated data lake that connects data across the buus business units while still allowing those business units to implement their own security policies and access controls um we were able to quickly and seamlessly stand up a large language model platform on top of our existing Databricks infrastructure rather than having to start from scratch because of the composability of this platform where different business units can use some parts of it but not all parts of it key components of our platform that we've been able to leverage for the large language model um use cases is um the datab breaks ai platform and model serving which allows us to manage calls from all model endpoints as well as um ml flow for large language model operations and governance which is a huge developer experience advantage since many of our machine learning engineers are already familiar with ml flow because of both the complexity of our different business use cases in the quickly evolving external landscape we have centered our ai strategy around supporting production quality use cases for ai while assuming that the models are going to completely change over the next few years our strategy has three key pillars first of all we center federation the idea that we want this consistent interface that we can swap models out behind as the models continue to evolve we also really center this idea of agility where we know that the patterns that we're using to call models today are not necessarily the patterns that we want to use to call models tomorrow and so we want our platform to be able to evolve and scale to support that and then finally um we really center this idea of control as a financial technology company we have a lot of really sensitive enterprise data that we want to make sure is staying secure as we use it for new use cases like large language models as innovation continues to happen in the generative ai space and new models get least we want to keep this optionality to easily switch out these models without having to write a whole bunch of new code for instance our company's philosophy is really aligned with open source but a lot of our original use cases have used proprietary models like gpt 4 with the Databricks u mosaic ai gateway we can really easily compare the llama open source models to open ai's closed source models and match the same easy developer experience without having the need to implement new apis this federated approach ensures that we stay agile and responsive to advancements in ai technology while most of our original use cases use state-of-the-art models like gpt 4 we're increasingly seeing that we have really specific use cases where we want to fine-tune open source models using proprietary data with mosaic ai training and Databricks we can easily fine-tune these open source models in the same place within the platform and most importantly that means that the data doesn't have to leave the platform which would be an additional security risk and then once this model is fine-tuned we can can easily serve it through that same ai gateway finally for most of our use cases we don't just use a model but we use this rag pattern that was mentioned earlier where we're sending a lot of context along with that model to make sure that we get the best and most relevant results with mosaic ai we can re easily implement this rag pattern right within the platform with a full and control and governance of components like unity catalog and not have to worry about the data leaving the platform this centralized approach really is important for our security posture to make sure that we can have the right granular access um as well as centralized concerns like compliance and cost optimization which can become really sprawling if we have a bunch of different decentralized endpoints using our flexible platform we've been able to see real change in metrics that directly impact the business we've seen a 26% improvement in the time that it takes to deliver a generative ai application to production we've seen a 32% increase in developer productivity with engineers who are using the platform and all of that has added up to around $10 million in additional productivity gain versus our original forecasts thank you and if you're interested in joining please check out our career [applause] page awesome that's super cool i really love working with block and square and especially jack's team you know they're super regulated right as a financial services firm but they're one of our most cutting edge uh partners that we work with okay they're using all the latest stuff and they're pushing us to the limit on everything uh and wow what an awesome demo by casey i was so nervous when she got that error i told people in the back no more live demos ever again and then she fixed it i was like okay more live demos uh awesome so it's a great pleasure to introduce on stage uh you know our uh professor f lee and she's going to be talking about what she calls spatial intelligence but you know i think of it as like world models you know llms that not just understand language but they understand the whole world so super excited about this so let's welcome fa to [music] stage thank you ai [music] hi good morning everyone really really happy to be here and uh uh i'm not going to show you products or live demos so i'm here to share with you a glimpse of the future a glimpse of the future that goes beyond just understanding language and i call it from seeing to doing so let me start by showing you something um actually please have my fonts a little larger um in fact the i'm going to show you nothing this is not a glitch um this is our world 540 million years ago pure endless darkness it was it wasn't dark due to the lack of light it was dark due to the lack of sight indeed sunlight filtered a thousand m below the ocean surface and light permeated from hydrotherm vents onto the seaf flor although briming with life there was not a single eye to be found anywhere in these oceans no retinas no corneas no lenses so all this light all this life went unseen there was a time when the very idea of seeing didn't exist yet when it was something that has simply not been done before until it was for reasons were only beginning to understand trilobites the first organisms that could sense light emerged they were the first inhabitants of the reality we now all take for granted the first to discover a world in which something exists beyond the self a world of many other selves this reality to see this ability to see is thought to have helped usher in a period called cambrian explosion where a huge variety of animal species entered fossil records what began as a passive experience the simple act of letting light in soon become much richer and far more active the nervous system began to evolve sight turned into insight seeing became understanding understanding led to actions and all of these gave rise to intelligence so half a billion years later we're no longer satisfied with just nature's gift of intelligence humans are now um humans are now on a quest to explore how to create machines that can see just as int intelligently as we can if not better nine years ago i gave a talk at ted and i deliver what was an early progress report on computer vision a subfield of artificial intelligence three powerful forces had emerged for the first time about about a decade ago a family of algorithms called the neuron network fast specialized hardware called graphic processing units or gpus and you're going to hear from jensen later and big data like the collection of 50 million photos that my lab spent years curating called image that when combined these factors cause computers not only to see better the ever but they also ushered in the age of modern ai we have gone we have come a long way since then back then a decade ago just labeling objects was a breakthrough like the first glimpse of light for those early trilobites but the speed and accuracy of neur network algorithm rapidly improved year after year the annual image that challenge led by my lab gauge the performance of these algorithms and every year the submissions broke records as you can see from this plot showing the annual progress and some of the milestone models they are really incredible but we have but we're not satisfied since then we have further developed models in our lab as well as other labs that can segment objects and recognize even the dynamic relationships among them in videos as shown here but there's more to come i remembered when i first showed the world the first computer vision algorithm that can describe images and photos in human natural language a way to do automatic picture captioning a caption writing that was join work with my brilliant former student andre kathy at that time i push my luck and asked andre to reverse this give a sentence and ask computers to generate photos and andrea said haha that's impossible well as you can see from this a recent tweet from him that just a mere few years later the impossible has become possible um this is thanks to the development of recent diffusion models used in generative ai ai programs can now take any human input sentence and create a photograph or a video of something that's entirely new many of you have seen the beautiful result of sora by open ai and many other companies recently but even without the enormous number of gpus my students and my collaborators were able to create a generative model called walt month before sora was released and here are just some of the results of course you can see we have room for growth and we do make mistakes i mean look at that cat's eye right it dips underneath the water without even getting wet i call it a catastrophe i hope someone is making better ai jokes for me but if the past is a prologue we will learn from these mistakes and create a future that we imagine and in that future we want to take full advantage of all that ai can do for years i have said that taking a picture is not the same as to see and understanding it now i would like to add on to that simply seeing is not enough seeing is for doing and learning when we act upon the world in 3d space and time we learn and we learn to see and do better nature has created a virtuous cycle of seeing and doing powered by spatial intelligence to illustrate what your spatial intelligence does constantly let's look at this picture raise your hand if you feel like this photo want to make you do something keep your hand up if this has actually happen in real life in the last split of a second your brain looked at the geometry of the glass it uh the the place in 3d space it's relationship with the table the cat and everything around it and you predicted what will happen next and this and then you will dive towards that glass to save your carpet this earth urge to act is innate for beings with spatial intelligence which links perception with action so to advance ai beyond what is capable of today we need more than ai that can see or talk we need ai that can do just like what nature did to us and indeed we're making exciting progress here our recent milestone in spatial intell ence are catalyzing that virtuous cycle of teaching computers to see do learn and then see and do better so this but this is not easy it took millions of years for animals to evolve spatial intelligence and in contrast it took only a couple of hundred thousand years to evolve language and that that evolution depends on the eye using light to project 2d images onto the retina and the brain translating those images into 3d only recently a team of computer vision researchers at google just did that they created an algorithm that can take just a set of photos and turn that data into 3d shape or 3d scene and here are more examples of that work in the meantime my students and colleagues are inspired by this work at stanford and went a step further and create an algorithm that only require one image to generate 3d shape like you see here and here are a few more examples of that recent work and recall we previously used text input to create videos a group of researchers at university of m michigan figured out how to translate a line of text into 3d room layout and you're seeing an example here in the meantime my colleagues and their students at stanford have developed an algorithm that can take an image and generate infinitely plausible spaces uh for a viewer to explore these prototypes are the first budding signs of a future possibility one where human race captures our entire world in digital forms and is able to model the richness and nuances of our world what nature was able to do implicitly in our individual minds spatial intelligence ai can now hope to do in our collective conscious as the progress of spatial intelligence accelerates a new era in this virtual cycle is playing out before our eyes this back and forth is catalyzing robot learning a critical component to any embod intelligence system that needs to directly understand and interact with the 3d world a decade ago imet from my lab enabled a database with millions of high quality images to help computers to learn to see now we're doing that with behaviors and actions that teach computers how to act in 3d world instead of manually creating training examples we now using simulation environments like the one provided by nvidia omniverse powered by 3d spatial models that offer endless varieties and interactions you're now seeing a small set of those examples of the infinite possibilities of training our robots in simulation environments in a project that my lab has been leading called behavior there there's also exciting progress in robotic language intelligence combining vision and spatial intelligence using large language modelbased inputs my student and i my student and collaborators are among the first to uh first team to show robotic arm performing a wide range of tasks based on verbal instructions uh like this one asking the robot to um open a a drawer but watch out for the vase or this one to unplug cell phone it's a kind of a unusual way to unplug but okay um and this one to put uh to make a sandwich and uh well typically typically i would like a little more on my sandwich but this is not a bad start so in that primordial ocean 540 million years ago the ability to see it and perceive one's environment set of a cambrian explosion of interactions with other life forms today that light is starting to reach digital minds just as it once did to our to our ancestors spatial intelligence technologies are allowing machines to interact with one another with humans and with the 3d world real or imagined with this future taking shape we can imagine how it will have a profound impact on so many lives let's just take healthc care as an example in the past decade my lab took some of the first steps towards applying ai technology to challenging impact uh to challenges impacting patient outcome and medical staff burnout together with my students and colleagues at stanford school of medicine and partnering hospital were piloting smart sensors that that can detect when a clinician enters a patient room without properly washing their hands keep track of keep track of instruments during surgery or alert care team when a patient is at physical risk such as falling we consider these technologies to be forms of ambient intelligence and these extra pairs of eyes can make a difference but i would love to see more interactive help for patients clinicians and caregivers who also desperately need an extra pair of hands imagine autonomous robots transporting medical supplies so that caregivers can have more quality time with our patients or augmented reality guiding surgeons towards safer more efficient and less invasive operations imagine patients with severe paralysis controlling robots with their thoughts that's right with brain waves so that they can do everyday tasks that you and i take for granted you're actually seeing a glimpse of that future now in this pilot study from my lab as you can see in the video here a robot arm is cooking a japanese sukiyaki meal controlled only by brain electrical signal noninvasively collected through eg caps so no chips or no electrodes were inserted into the person's brain this entire robotic a action is done by remote brain control thank you half a billion years ago the emergence of vision not only turned a work world of darkness upside down it also kicked off the most profound evolutionary process the development of intelligence in the animal world ai's breathtaking progress in the last decade is just as astounding but the true digital cambrian explosion won't realize its fullest potential until computers and robots have developed the kind of spatial intelligence that nature has endowed to all of us it's now time to train our digital companions to learn how to reason and interact with this incredible 3d space we call home and to create many new worlds for all of us to explore realizing this future won't be easy and it will require all of us taking thoughtful steps to develop technology that can always put humans in the center if done right computers and robots powered by spatial intelligence will not only be useful tools but they they can also be trusted partners that can augment and enhance our productivity and humanity while respecting our individual dignity and lifting our collective prosperity what excites me the most is a future in which as ai grp grows ever more perceptive insightful and spatially aware it joins us in our quest to satisfy our curiosity to always pursue a better way so we can make a better world thank [music] you awesome thank you wow that's so awesome i love the you know telling andre carpati like after he did all this research like hey now do it backwards do the reverse just take the text and produce the images wish i could do that okay here's the audience put together a whole show use holograms figure it out do the announcements and be done awesome so i am super super excited to introduce our uh next guest uh who actually is a man who does not need any introduction so i want to welcome the world's one and only rockstar ceo nvidia jensen wang to stage [music] [applause] [music] [applause] thank you so much awesome thank you for coming so uh matt i just want to start uh you know just looking at nvidia's amazing performance you know three trillion dollars like did you did you imagine it would be this way say five years ago that the world would unfold this way sure from the very beginning it's so awesome to see any advice for a you know fellow ceo how do we get there what whatever you do don't build gpus okay all right let me tell the team we need to back out awesome man uh so we spent a lot of time this morning uh talking about data intelligence which uh by by what we mean enterprises you know they have all these proprietary data training ai models that's customized on their data that they have how important is that is that something you see you know well you know is that something that we need to invest more in what what are you hearing well every company's business data is their gold mine and there's every company is sitting on gold mines if you have if you have a uh a flywheel of services or products customers enjoying those services and products giving you feedback you've been collecting data for a long period of time it could be customer related it could be market related it could be supply chain related um all of us have flywheels of data that we've been collecting for a long time we're sitting on mountains of it but the fact of the matter is none of us have really been able to extract insight or even more importantly distill intelligence out of it until now and so we're pretty fired up i know we are yeah and we're using it in our chip design we're using it in uh our bugs database uh we're using it uh in creating new products and services and we're using it in our supply chain and you know for the very first time you we now have a business we now have an engineering process that starts with data processing and refinement and learning models and then deploying the models and connecting that fly wheel collecting more data isn't that right yeah and so we're doing that in our company and is make making it possible for us to frankly be one of the largest small companies in the world and and the reason for that of course is because we have so many ais in the company helping us out doing amazing things and i think every company is like this you know and so so i think this is just an extraordinary time and it starts with data it starts with Databricks that's awesome thank you so much um curious you know there's this whole debate brewing uh closed models versus open source models you know are open source going to catch up is you know are both going to exist is it going to eventually just be dominated by you know one giant close source model uh what are you seeing what are you thinking about the whole open source ecos system and how important has it been for sort of development of llms and how important is going to be going forward well we need we need frontier models we need amazing frontier models of course the work that uh open ai is doing the work that google is doing um uh really really important and pushing the frontiers and and helping us helping us discover what's possible uh but if you were to to look at this year probably the most important events this year were related to open source uh llama 2 now llama 3 uh mraw uh the work that you guys did Databricks um uh dbrx do i have to say dbrx dbrx right dbrx dbrx dbrx i think really really cool stuff and the reason why it's so really cool is because it activated every single enterprise company it made it possible for every company to be an ai company isn't that right you're seeing this yourself and we're seeing this all over the place uh we recently turned llama 3 into a a fully containerized uh inference microservice and it's available for downloads you can go to hugging face you can go to of course uh Databricks and it's now it's now being integrated into several hundred companies around the world and so that tells you something about how open source has activated every company to be an ai company we're using open source models all over our company and and we create some propr rary ones uh we uh uh uh fine-tune um open- source ones uh train them for our data and for our skills and so so i think without open source it wouldn't have activated this entire global movement for every company to be an ai company i think it's a huge deal yeah that's super awesome uh so both are going to be around and we need both like open and close uh and this is is this the nim framework you're talking about nims uh of how you do the survey yeah we call n yeah yeah yeah we're super excited you know i'm super excited to announce here that we're going to put dbrx inside nims and we're going to serve it on datab breaks and actually any new models that we develop in the future so we're super excited about ns yeah it it's it's actually quite quite an amazing thing in order to in order to create one of these one of these endpoints these apis these large language model apis the stack is really complicated you know these are giant models even though even though you know they they seem small these days they're still computationally really large and the computing stack is really complicated they're hundreds of dependencies necessary to create one of these endpoints yeah and so uh we created this thing called the nvidia inference microservice where we package up all of the dependencies we optimize all of it we have a factory in the company with all these engineers working who are expert in doing this and we package it up into a microservice and you could enjoying it enoy at at Databricks uh you could u download it and take it with you uh you can fine-tune it with uh microservices that we call nemo and uh use it anywhere you like it runs in every single cloud runs on prem uh you know you can enjoy it everywhere that's super awesome amazing thing yeah yeah and it's awesome you can even run it on prem i mean that's that's you know it's not you don't have to be on the cloud um that's super awesome okay so when we talk to customers we're hearing that you know they have to develop this uh sort of expertise in house to customize models to gain advantage what are your thoughts on that well i think i think in the future um look what's happening in the world today is that that we figured out a way to tokenize almost any information almost any data and we can extract structure understand uh learn its representation understand the meaning of that information um of almost any kind it could be of course sound speech words language images videos uh it could be chemicals and proteins it could even be robotics articulation and manipulation it could be steering wheel articulation driving uh we can tokenize almost anything and because these these uh these uh cloud data centers um are really producing tokens we manufacturing something that is quite unique for the very first time you have this instrument called these ai supercomputers that we build uh it's producing tokens yeah generating tokens in essentially a uh factory that's designed for that one job and this ability for us to manufacture intelligence at scale is pretty new and that's one of the reasons why i'm almost certain now as we're building these ai factories everywhere uh for all these different industries that we're in the beginning of a new industrial revolution instead of generating electricity uh we're generating intelligence every company of course at its foundation is about domain specific intelligence yeah uh very few companies on the planet knows more about data and data processing and ai and uh the infrastructure necessary to do all that then Databricks um we are quite specialized in the work that we do and we're uh at this foundation all about that domain specific intelligence every company is it could be financial services could be healthcare whatnot and so at the end of the day every one of us will become intelligence manufacturers and if you're going to be intelligence manufacturers today you have hr in the future you're going to have you know hr for ai and we call them ai factories yeah so every single company will have to do that we are doing that you're doing that um uh we see companies large and small doing that and so in the future uh % of us would do that uh you start with of course your domain specific data it's sitting in Databricks somewhere you're going to process uh that data and refine and extract intelligence out of it you're going to turn put it into a flywheel you're going to have an ai factory all of us will yeah this so so awesome i totally 100% believe in this and one thing we're excited about is you know so we do a lot of data processing and data processing it's like massive amounts i think we process about four exabytes every day you know 4,000 terabytes every day in Databricks and you know it it is the single largest computing demand on the planet today processing data yeah right every single company does it yeah exactly and you know it's actually highly paralyzable you know we do the same operations again and again again so i'm really really really excited to partner together uh to really bring that kind of gpu acceleration to data processing so we can do the same revolution that ai models have seen on the core data processing so we're super excited to partner with you on using gpu acceleration for our photon engine to be able to really kind of enter this new era of also applying gpus to core data processing right these massive workloads that today have to run on cpus getting them also run on nvidia gpus we're very excited about that yeah this is a big by the way this is a big announcement yeah the two the two most important trends in computing today is accelerated computing and generative ai yeah nvidia and Databricks are going to partner to combine our skills in these areas to bring it to all of you and [applause] yep and this this work in accelerating data processing as you know it's it's highly paralyzable yeah but it's really arcane it's really complicated and the reason for that is just there are so many data formats there's so many different ways to group and join and you know just wrangling data is a really complicated suite of libraries spark is a super complicated suite of libraries and it's taking us five years working around the clock to finally have a suite of libraries that can now accelerate photon and this is such a big deal we've been working on this for a long time for many years so now we're going to accelerate photon and make it possible for all of you to wrangle data process your data a lot faster a lot more cost effectively and very importantly consume a lot less energy yeah makes a lot of sense a huge deal y it makes a lot of sense right because it's uh in the end of the day even though it's very complicated and you know it has a lot of corner cases it is highly paralyzable and uh you know it is specialized still it's not you don't really need generic compute for that right it's it's we want to do it's like you know same thing again and again and again on xaby to data right it's not we're not doing xaby to data that's completely unique so i'm very very excited about this and i think it's really has the ability to revolutionize and really bring faster you know performance lower the cost and just you know uh it's going to be amazing yeah look look what happened when we're able to process enormous amounts of data so quickly it made it possible for researchers to one day wake up and say guess what let's just go get all of the data on the internet yeah and train a giant model because it doesn't take that long without acceleration without accelerated computing nobody would have ever conceived of doing that it would have been way too expensive or would have taken too much time but now you know it's kind of a mundane thing to do and so you know we're going to be able to process exabytes and exabytes of data so much more cost effectively and so much more efficiently from a time perspective imagine all of the ideas that you're going to have it's you know it's going to be hey let's just take all of the data of our company and we're going to train our super ai you're going to do it yeah if the day is going to come yeah i mean it was a sci-fi idea right to take the whole internet nobody thought you could do it we needed the hardware to get there the infrastructure to be there so we could specialize it and now you know everybody's doing it um so i want to switch gears god i love myself just kidding just kidding we love you too uh so um i want to switch gears um so you know this generative ai boom has been amazing um you know but the early days you know most enterprises started with chatbots let's build our own chatbot you know customize it on our data and so on but now we're seeing people branch out to more and more sophisticated use cases what new applications in ai are you the most excited about going forward um the number one most impactful will probably be customer service for all of the enterprises that are here customer service you know represents probably several trillion dollars worth of expenses and every company has every company has it every single industry has it every company has it um and and um the important thing about the chatbot the customer service is is partly about the fact that you could automate but it's mostly about the data flywheel yeah you want to you want to capture the the the conversation you want to capture the engagement in your data flywheel it's going to create more data of course we're probably you know right now we're seeing data expanding about growing about 10x every five years wow i would not be surprised to see data growing 100x every five years because of customer service and so we're going to we're going to connect everything into a flywheel it's going to collect more data capture more insight we're going to extract better intelligence out of it which will provide better service maybe it's even more predictive in the sense that proactive in the sense that uh before a problem even arises you reach out to the customer and say you know this thing is about to expire or um we notice that you're still using this version or whatever it is and you reach out to the customer and proactively solve a problem just like preemptive maintenance we're going to have pro proactive customer support which is going to create more data we're going to write that flywheel and so i think i think customer service is probably going to be the most uh profoundly uh supercharging capability for c for for most companies because of that because of the data it's going to collect but we've tokenized everything you know the i'm excited about the fact that we're generating chemicals we're generating proteins um uh were uh carbon capture materials uh carbon capture uh enzymes uh in incredible batteries that are being being uh designed uh and so we're generating physics uh phys physical ai uh and recently we um made it possible to do regional weather prediction down to a couple kilometers now it would have taken a supercomputer about 10,000 times more capability to be able to predict wea down to a kilometer and now we're doing we're using generative ai to do that wow um and and so as a result uh logistics will be enhanced insurance will be enhanced uh of course uh uh keeping people out of harm's way will be enhanced um and so so physical things uh biological things uh of course uh you know generative ai for 3d graph graphics digital twins uh creating uh creating virtual worlds for video games i mean generative ai is just uh everywhere every single industry if your industry is not involved in generative ai is just because you you haven't been paying attention it's everywhere yeah yeah i totally believe you know we're going to see there's no no area where we're not going to see applications of this makes a lot of sense it's so exciting um you know these new frontiers are super exciting and there's huge needs for data you know ai what's your thoughts on how we can help enterprises make ai that's more sustainable well um there's a lot of sustainability has a has a lot of lot of different perspective one one of the sustainability uh has to do with energy yeah and and um remember ai doesn't care where went to school you we could we don't need to put ai training uh data centers uh near population where the energy grid is challenged already we could put it somewhere where it's not challenging and so uh you know that the world has earth has a lot more energy uh it's just in the wrong places and so i think for the very first time we can go capture that excess energy compress it into an ai model and bring these you know ai models back to society where we where we could use it uh that's one one one major thought and another is um i remember that ai is not about training ai is about inference yeah and it's about the generative capabilities of the ai you're training the model so that you could use it uh and when you think about the longitudinal benefit of ai and i just gave you the example of of u predicting wether using ai instead of using supercomputers we understand basically the laws of physics that's involved in weather prediction we don't need to simulate it from first principles every single time we got to generate it using ai and by generating it using ai not only do we reduce the amount of time that it takes improve the resolution that we can generate for but also the amount of energy by thousands of times not tens of you know not percentages thousands of x factors well by doing that we're doing the same thing by designing chips that you're using in cell phones um say you know you you train the model ones you design better chips with those models as a result you save energy for everybody involved um just when you think about the longitudinal benefit of ai i'm fairly certain that it will demonstrate the amount of energy that's saved and then one last thought about about generative ai you know that today and why is such a big deal from a computer science perspective today's computing experience is retrieval based largely you know we touch the phone and even though uh when we when we use our phone we think it uses very little energy every single time you touch it it goes off and sends rest activates rest apis all over the world retrieves information the internet's on you know lit up uh brings back a little bit of information for you from all these different data centers assembles it based on a recommender system presents it to you well in the future it's going to be more contextual more generative right there on the device running a model a small language model the amount of internet traffic will be dramatically reduced and it'll be much more generative with some retrieval to augment right and so so the the balance of computation will be dramatically shifted towards immediate immediate generation well this is very you know it's going to save a ton of energy and it's very sensible and the reason for that is this imagine every single question that ali asked me i got to race back to my office go get some files bring it back and present it to him let him decide which piece of that information he wants to extract for himself instead i'm generating everything you know from about 25 watts right now as we speak yeah right and so the amount of energy that we save is going to be extraordinary and the computing model is going to transform completely and so this way of this way of uh of computing is going to save tons of energy uh of course we're going to get our answers a lot more efficiently instead of us combing through stuff but then we'll have even more questions right we'll have more questions which is really in in fact that's the big idea the big idea about the future as working with ais is prompting we're going to have so many more interesting questions because we're going to get a lot of answers very quickly yeah so this is a very big deal yeah very exciting future okay my final question to you how do we help customers you know organizations here get started today what's the best way well you know i i told you before that i thought the p pivot um of Databricks expanding from data processing to data governance and uh store uh and then extending it into all the way uh longitudinally all the way to extracting uh intelligence out of that data i think that that was completely genius and and uh i forget her her name but i thought cookie lady did an incredible job casey what's that casey okay don't steal her [applause] please i i thought she did an amazing job i was enjoying i we were in we were backstage and and everybody wanted to talk but i just wanted to watch her give her demo i i thought i thought the platform uh is incredible and you've made it easy for people to uh manage their data extract information process that data wrangle that data you know wrangling data is still a very big part of training the model people talk about training the model but long before you train the model you got to go figure out what data right it's about data quality it's about data format it's about data data preparation and so so i i think i think the way you start is is uh come to Databricks and uh use the use the Databricks uh data intelligence platform am i right yeah absolutely [applause] who who wouldn't call their platform dip that such a good idea you know dip Databricks dip sounds good i like it it's almost as good as nims all right all right there you go there go well you can do both together right yeah you don't have to pick go go get yourself a nim on dip yeah i agree why not that that's the way to do it that's that's dip that absolutely start whatever you do just start whatever you do start you have to engage you have to engage this incred ibly fast moving train remember generative ai is growing exponentially you don't want to wait observe an exponential trend because in a couple of years you'll be so far behind it's incredible just get on the train enjoy the train as it's getting faster and faster exponentially learn along the way and so you know this is this is one of those things you can't learn by watching you don't want to learn by reading about it you just learn by doing yeah and which is the way we're doing it and so just get engaged all right that's great advice jensen it's been an amazing decade thank you for everything we've been great partners looking forward to our next decade together Databricks all [music] right all right okay so uh we're going to shift gears okay so we're going to go back to core of data platforms what do data platforms do data processing we're going to talk about data warehousing okay so i'm super excited to welcome on stage my co-founder rold chin he's been actually leading the data warehousing revolution on Databricks and you know assembled worldclass team and they having amazing results every month they keep improving the platform making better faster cheaper so i want to welcome to stage rold [music] chin thank you ali all right morning it's a little bit hard to top off after jens and ali um so we announced dat break sql the private preview of it about four years ago and um ever since we've been humbled by a reception dat bre sql become the fastest growing product in the history of datab breaks and today over 7,000 customers worldwide large and small including shell at&t adobe are using Databricks sql for their data warehousing workloads on datab braks but one of the fundamental reasons why Databricks sql is taken off so quickly really goes back to the idea of the lake house itself before lak houses here's what a typical enterprise data architecture would look like you might have one or multiple of data warehouses for your business intelligence workflows and what i call looking back in time and you have one or maybe multiple of your data lakes used by your data scientists data engineers and um ai engineers for building machine learning models looking into the future and the two dispar stacks were really incompatible they have different governance models they have different storage formats proprietary versus open and that led to a lot of data duplication data silos and honestly a governance nightmare so when we looked at this problem um a little bit over four years ago we came up with this kute little term called the lak house and the idea of the lak house is fairly simple at a high level let's marry the best of both worlds from data warehouses and data lakes and combin into a same package um but the technologies weren't completely in place back then we have to build a lot of stuff we have to create delta lake for the foundational storage layer we have to create unity catalog for the governance slayer but over time the writing become on the w the lak houses will be the future and um even proprietary warehouses started talking about lak houses forester a leading anos firm even uh come up with a whole new forester wave called the lakehouse and we're very proud they breaks the by far the leader on the forester wave now going back to data warehousing one of the most important workloads on the lak house is the ability to support data warehousing workflows dat break sql is our product to support that um some of you are among many of you are among the 7,000 customers been using Databricks cle every day some of you have never tried it some of you might have tried it three or four years ago um and have form your impression since you might still have this impression that Databricks is great for data engineers for data scientists for the sh per technical people with phds and computer science but not necess for all your analysts or your business users but what we've really done is we've changed so much of the platform from every single layer that now the platform looks completely different and to give you one example would be when we first release datab break sql it would take about 370 seconds to acquire a warehouse for compu but today that number is down to less than five so that's a more than 70 times improvement just in three years and that's just one example we looked at what are the most important fundamental areas and we picked three and we really worked on every single inch of it and this includes core data warehousing capabilities out of the box performance and ease of use let's get started with core data housing functionality of course to support data warehousing workflow we needed a lot of features and functionalities that were not available in the lake house we need n full nc sql support we need the materialized views we need the ro based access control this were not features were available out of the box back then so we built all of them all right uh and building on top of those uh we now have built a very large data and ai partner ecosystem so all your favorite tools were were out of the box for data warehousing and especially in the area of dat business intelligence tableau powerbi thought spot looker sigma click all those just work out of the box on the datab brook sql and this really substantially lowers migration cost from traditional data warehouses over to Databricks sql the second area that we spent a lot of time on is price performance or out of the box price performance and price performs one of the most important typically one of the most important evaluation criteria in data warehousing poc's and for two reasons one is data warehouses tend to be one of the most expensive if not the most expensive business systems out there you spend a lot of money on it to be able to save on it is essential to your operational efficieny and the second is you want to guarantee all of your analysts and business uses have the best class in best experience when monday 9:00 all of them come to work start opening their dashboards thousands of queres hitting the thehouse you want to guarantee predictably low latency so one thing i hope was clear from past data and ai summits is Databricks has always been really really good at etl performance last year this is not a new chart last year we published a study that we compare Databricks sql versus a leading cloud data warehouse on how they would perform as we vary and grow data volume uh data in size so we grew from so we started with 100 gigabytes we grew all the way to 30 terabytes we just benchmark how well etl performance work um again against this two different platforms initially the two compare fairly similarly when the data set were small but as you scale the data set the economics of Databricks sql really start to show because Databricks sql scale lot roughly linearly in terms of cost whereas the data warehouse scale exponentially and it might sound surprising but in practice it's actually not that surprising you think about it the whole foundation of data warehouses were built to handle initially transactional business data out of oltp databases which t to be relatively small so the whole system designed from ground up to be really optimized for smaller amount of data and when you do poc's you typically don't load enormous amount of data into the warehouse you grew your data over time so they really tried to optimize with earlier stage but in Databricks sql because we come from a more of a data lake heritage we really make sure the whole system was scale far beyond 30 terab even to pedabytes range now at last year this conference i gave a very different style of keynote i gave a whole technical talk about how we using ai systems to improve data warehousing engine performance um at the time we knew ai would be something but we didn't have the idea any idea how big and how important it would be um in the last 12 months we revamp almost every single layer of our engine to incorporate ai systems in them um and this goes all the way from a bottom physical data layout the middle layer create engine and the top workload management scheduling um and and then we're really seeing with our own eyes how much the ai systems can improve and honestly i think most of us myself included underestimate the impact ai system would have um now you at this point you're curious hey so tell me more show me the actual numbers but before i do that let me just show you a couple examples of things we've done to give you a little bit more concrete idea and some of these examples might even sound abstract and difficult to understand but that's okay right uh the whole point is you don't have to understand all of it but you can just see the benefit with your own eyes the first example is uh michael would tell you more about in the delta lake talk tomorrow about liquid clustering which is a new fundamental breakthrough in uh data clustering but on Databricks we have built ai systems to learn based on your workloads and automatically actually suggest and cluster data for you not only does it suggest the type of columns um you can cluster by but it also actually uh decides should it run all the operations online as part of your etl workflows should you get the fastest time to us clustering or does it run it offline with minimum impact your etl workloads the same ai system is also applied to statistics which is one of the most fundamental ingredients in querer optimization for warehousing um the ai system would actually decide what type of statistics do you need on different type of columns how do you actually minimize amount of overhead do we apply higher fidelity statistics or do we apply actually more sketches depending on thetive workflow you need and one concrete example i gave you last year for some of you that remember is this thing called predictive io and at the time i joked about the term indexes indexes which is very difficult to pronounce uh the the whole idea is predictive io applies a machine learning model to predict where the data would be and give you the benefit of indexing without the overhead like rewrite amplification of indexing um thanks to the mosaic ai stacks uh improvement over the last 12 months we now can imp a model that's an order magn larger um in terms of parameters and feature vectors in predictive io so we just roll out predictive io 2.0 this is not something you have to optain it just works automatically under the hood um but it can accelerate way more workloads beyond the simple selections i've showed you last year um so okay numbers um there will be a lot of numbers in the rest of the talk um you've all seen charts like this um a vendor showing s of some benchmarks and beating a compan by a white margin right every single vendor talk you've been to show that uh i'll show you some talks uh dex charts like that too but this will be the first time i bet you seen um a vendor's own chart uh in which the vendor didn't perform favorably so um i think it's sort of conventional wisdom if you have a pd in Databricks optimization you could get the best price performance out of Databricks one way another but what i really want to make sure is if you don't have time or you don't have that expertise to optimize your system can you get the best performance out of the box without doing anything so we created a synthetic benchmark exactly for that scenario which is continuously ingest a lot of data and then um we run the tpc queries queries against them and when we first did that benchmark in 2022 about two years ago um the data warehouse we benchmarked earlier was actually doing better than Databricks sql last year when we incorporated more of the ai capable abilities the two systems got to roughly on part but something fairly magical happened last year um in the course of last 12 months as the ai systems evolve and become larger and larger and more sophisticated and as you learn more and more workloads it actually got 60% faster out of the box compared with 2022 and it now has industry-leading performance and this is complete outof box no tuning required right now some of you would ask hey you started the talk by talking about performance and thousands of analysts hitting the system but that benchmark seem to be about running individual ces so we also created the scenario a benchmark for the scenario really matter which is 9 9:00 a.m. monday everybody opens their dashboard um in this benchmark we created a bunch of robots that just keept hitting the warehouse using a variety of bi queries robots don't take coffee breaks so each robot's probably like 10 times the normal human being um and we again just compare a leading cloud data warehouse versus Databricks sql um when there's slower degree of concurrency d sql and the leading warehouse would have roughly the similar um low latency but as we continuous scale and adding to 512 users the difference really started to show the warehouse had trouble scaling um to the large number of users and 512 robots probably simulate tens of thousands of users at this point where datab bri sql continue to deliver the same low latency um now some of you actually all of you probably also think hey i don't trust benchmark to show even though you say you simulate real world scenario but i really only care about my own workloads my own real world scenario that's exactly what i tell s our engineering team when they show me all this benchmarks um i said hey i really only care about how our customers queries would actually uh perform so we took a step at this we looked at all the queries all the bi queries are repeating on dat bri sql to establish a baseline in 2022 about two years ago and we measure their performance over time we track them for the same query how well they performing over time and today this year in 2024 the same bi queries would run on average 33% faster what does 33 or 73% faster what does 73% mean a tw that used to take 10 seconds now takes 2.7 seconds it's almost 4x faster and the best thing is this is without you having to pay a single di more without you having to do anything it's just the system getting better and better under the hood and it's because how much obsess and how much we know performance matters to you the last part is a use of use um again i said earlier so you have the impression that Databricks is great for data scientist is data engineers but not necess great for analysts um so in the last few years heard a lot of feedback we revamp the user experience completely but there's really no way for me to show you um how much different you've gotten you really have to try it yourself all right the best i can do is show you a few screenshots that demonstrated before and after one example would be hey if you want to look at the data lineage or history um or how tape data are created in the past you have to create run a sq query to get a tab result but this year all you need to do is the system will show you a visually how the entire end to-end lineage of all the data sets and not just for table was also machine leing models error messages used to be daunting to the last technical user engineers might love it because they see sack traces they point out exactly which line number have the issue but it's very daunting to business users um now we add actually output a really simple error message error codes that can google but even better the ai models on the side would actually recommend and automatically fix error for you we also add a lot of quality of life improvements when it comes to n uh sql features themselves beyond what standard sql will give you this is like sql udfs lateral column meduses session variables all the features once you start using you ask yourself how come not every warehouse has this feature what have i been missing out on in the last like 10 years um the and but of course what we really thing we can le frog is uh through the use of ai dat brs assistant is extremely popular has been extended to cover all cases of sql um we introduced ai functions in db sql so all of your analysts can now get um the full power of off the-shelf both open source and proprietary large language models out of the box but even better your ai engineers can build a new model publish it to unity catalog and make it immediately available in dbc for all your analyst consume same thing is applied to mosa ai stack vector search dbc can now actually create the vector index automatically now rather than me showing more about this i would like to invite pearl onto stage to show you a live demo of what all this look cl [music] per thank you thank you reynold so as reynold shared we're using ai to better your Databricks sql experience by making the leg house much simpler and more powerful so right now i'm actually in the sql editor and i have some revenue data for a few shops plotted as a time series graph and this is great and all but i would love to see what revenue could look like several months from now better yet it'd be great if i could use ai to predict this so i actually can with our ai functions and i'm going to go ahead and use the assistant here to help me do exactly that so i'm going to add a forecast through of september 2024 all right so it looks like the assistant took my original code here and it's uniting it with the forecast results and it's doing that by just doing a simple function call to my ai forecast function this looks good so let me accept it and hit run just like that i have my revenue forecasted no python or data scientists needed but this revenue trend it's kind of bad and i definitely want to share this with the management team with their bi tool of choice which is powerbi so i'm going to use the assistant again here and i'm going to create a materialized view that i will share to power or publish to powered bi all right assistant hard at work i'm going to accept that and hit run so why a materialized view materialized views are really great because it accelerates queries especially those that use ai for downstream users okay my materialized view is done and it's actually a govern object in un need catalog and i can search for it here all right let's go ahead and publish this bad boy to powerbi as you can see we have some offerings for tableau as well and i'm going to leverage the um Databricks sql compute engine i'm going to direct query to powerbi and this connection is made by securely through my sso login i'm going to select all the columns here build a column chart let me resize it and this can be shared with the management team now so let's head back to the sql editor and do more ai powered analysis so here i already have a query built out and it's going to show me a bunch of yelp reviews and i'm actually going to go ahead and use ai query which is actually an ai function here and it's going to call a custom model that's going to build responses to those reviews ai query is great because it also allows you to call foundational models like dbrx but also external models as well so here i have all my reviews um along with the responses from my custom model but i'm more focused on the reviews i kind of want to see how people feel about our food and drink so let me add a filter here all right food it looks like the filter gave me an exact match for the word food that's perfect let's try drinks there's no word match but that's okay because thanks to our new vector search function we can actually use we can actually search using an embedding model for related reviews um in regards to drinks and this is awesome because it doesn't have to be an exact word match as you can see i have brw espresso i have a rich cold brew a brw cappuccino these are all thanks so what the filter couldn't do vector search can so i'm going to build a parameter here Databricks makes it super easy to do so and i can easily search for uh reviews about any item for example here i'm doing pastries all right so we have all the reviews here but i need to do a little bit more analysis this is kind of a lot so i'm going to use an ai function as you can see we have quite a few to choose from from fixing grammar to translating text but i'm going to use analyze sentiment to perform sentiment analysis on the uh review column okay let's run that so this should give me another column that has sentiment a sentiment value on it and it looks like it does i have my review along with my sentiment i literally just built a mini rag and sequel so now i'm just wanting to plot this so i can understand the distribution of sentiment i'm going to hit save and it looks like pastries has a pretty good reputation but let me try service quality the store will need to know this to improve if needed and this viz makes it super easy so it looks like service could definitely use a bit of improvement look at all those negative reviews so to wrap i shared with you how Databricks sequels data warehousing capabilities allow you to query fun ai functions call models and make your data work for you so with that let's get to work back to you [applause] [music] reynold thank you pro all right so in the last four years we really revamp the entire experience of data warehousing Databricks seal today look nothing like the Databricks sql when we first released it we made it we implemented almost all the core data warehousing capabilities so you can easy lowest cost migration over to Databricks sql um we provide best in class out of a box performance and we also made it dramatically easier to use the datab brakes no longer just a platform for data scientists and data engineers but also all your analysts and all this are really possible um with build by ding on top of data intelligence stack and when you combine this why we say the best data warehouse is a lake housee because all of us rather stay in the open area environment of the lakeh house rather than being trapped in the data warehouse we feel a lot more productive like that in this case the lake house is also a hell lot cheaper than the warehouses thank [applause] [music] [applause] you awesome all right so uh i'm really excited about the next talk okay so a year ago um over a year ago we asked a team that uh you know was not really sort of uh working on the data bring stuff to like sort of splinter off and we said hey imagine you're not working at Databricks and you had to use generative ai to completely disrupt and build something from scratch and the next talk is going to introduce that product okay so i'm very excited to welcome on stage ken [music] wong awesome do this boom so as ali mentioned at the beginning of today our mission at Databricks is to democratize data and ai for everyone now for most people using data today basically means using reports and dashboards reports like this one now this is an actual real dashboard that one of our product teams have built it's all blurred here so you can't see what it actually says but it it started off as a simple straightforward dashboard that told us exactly what we needed to know about the state of the business just a simple straightforward dashboard but as we started looking at the data there we had new questions and we built some new queries and built some new visuals and we started adding it back into the dashboard because all that query building took a lot of time and over time we added them back and it started looking like this it was sort of a hot mess and well this is sort of the fundamental challenge with dashboards and bi in reporting today is you have to know the questions you want to answer and you have to build them into the asset in order to answer those questions now in the last few years there's been this excitement about using ai to solve this problem and when llms came bursting into the scene everyone in the market started just rushing to bolt on some off-the-shelf llm to add ai assisted capabilities the idea super super compelling right so you you saw how great the assistant was at just writing sql so if you just bolt on you can create an experience where the user can just ask questions in natural language and just get answers and now everyone in the industry has a demo like this right you can just ask questions and get a beautiful dashboard no more data scientists no more data analysts everyone gets the data they need but is that what's really happening in the real world well we took a look and we tried some of the leading bi tools that are available today including what one that i i personally think is really leading the field especially in this area and we used the realistic sales opportunity data set and asked it a simple question which is how's my pipeline how's my sales pipeline in this case now these are some of the answers that we got one vendor showed us that we had a whole bunch of nes which was truthful to the data right we did have a bunch of n but this was not exactly useful another tool told us that well i have no pipeline at all and the reason for that was that it generated some you know realistic looking sal stages and thought that thought you know we should have but we didn't have those and so it's told us we have no pipeline and a third vendor told us well just try again i don't actually understand your question because you never defined what pipeline was in the semantic model underneath our tool and so so we don't know what pipeline means now i'm showing you this not not to make fun of these tools because these these are actually great be tools and i bet with a little bit of work in in wrangling or semantic modeling we can get all these these tools to work and answer this question but my point is actually much more fundamental which is this idea of just bolting on a generic llm simply isn't enough uh to realize the transformative power of ai and bi together right and that's because generic llms just don't understand your the uniqueness of your data you don't understand how messy your data is and all the tricks that your analysts and data scientists have put in their dashboards queries and notebooks in order to handle it and they also don't understand the domain specific semantics that you have you know jensen talked about a little bit earlier right and every company has their own concept of things everyone knows what churn is but the churn is different for every single company now the tradition bi approach to solving this problem is to stuff everything and pre-model it inside a semantic layer and semantic layers are great right semantic layers are super powerful um but at the same time the reality is trying to model everything in your enterprise is also just not realistic and the good news though is that we think ai actually can solve these problems but it requires a groundup approach not bolting on something starting from the grounds up which is what we've done and that's why i'm so excited to announce today that we are launching Databricks aibi and ai first approach to business intelligence now we think this is really the first step in really truly democratizing data and analytics for everyone so what is it what is aibi well it's bi so it contains dash boards and our goal here is just to simply provide the quickest simplest way to build a dashboard in Databricks and share it with a lot of people now aii dashboards might not have all the bells and whistles of our amazing bi partners but it covers the basics it has a no code drag and drop experience it has scheduling it has exporting it even has cross filtering as the animation is showing here and it's all built to be super fast and deeply integrated into unity catalog so and and it's built into datab break sql so you don't have to manage some separate service but the really cool part of ai is that with one click you can flip to the other experience we call it genie now genie is a conversational experience similar to chatting with an analyst or type chatting them on slack and it takes in regular uh regularly express business questions in english and answers them back in vises and in queries now that might sound almost exactly like the same pitch that i was just telling you about uh so what what makes genie special well i think you guys are all experts in compound ai systems now because we talked so much about them but it's an a compound ai system that continuously learns the unique data and semantics of your your business it uses an ensemble of specialized ai agents that uses different llms working in concert and the these agents are able to leverage the tool and context that's available as part of the data intelligence platform so this includes things like unity catalog metadata like pkk constraints all the comments that you've put in there it also includes the execution query history of all your different workloads and this is how it's able to pick up contextually how all all the unique logic that your business uses it also uses related assets and look at look at things like notebooks dashboards and queries to understand the context as well and if a semantic model is available it'll also use that now all of this is connected into in an agentic workflow and and we've taught it in such a way that it goes back to the user to seek clarification when it's not able to infer the answer from uh every all the tools that's available to it and the really special part is it remembers these clarifications so it continues to learn and get better and better so that it's able to actually answer really complicated questions far beyond what you could pre-model in a semantic model um as as it kind of learns over time that's a lot of slides and it's very abstract so the best way to see what ai is is obviously to see it so i'd like to invite miranda luna product lead for aibi to the stage to show you [music] thank you ken hi everybody my name is miranda and i'm a product manager here at Databricks and it is my distinct pleasure to show you aibi in action today so why don't we do let's do the bi side of the house first um you know dashboards are part of our lives this is some crm data uh they're not going anywhere we are really excited to move beyond there but i'm just going to create this guy real quick um let's do a bar chart of opportunities by region perhaps so let's see let's ask for a bar chart uh pipeline region shorter accept it and then let's also do this little switch on the side here if we want to point and click so you've got options but if i want to break glass and go to the actual sequel let's get some comments here so this is the same guy pearl showed us i'm going to accept it so now this query is doing a couple things right it's helping clean up some messy data it's defining pipeline is stage 2 3 4 and it's getting all that context from the platform right it knows this because it was in a notebook we're a little behind today so i'm going fast so i'm going to publish and then if we come on over here that's what you expect to see in a bi tool i'm going to go ahead and make sure that we're moving ahead so we are on the canvas and perfect okay so we're going to go ahead and scoot on over to the canvas and this is where i'm going to go just double check that we have everything we expect from the dashboarding layer so we're going to go ahead and publish there we go and now we're going to go ahead and take a look at what we have here in publish mode now of course i can share this dashboard with anybody in my org you know i can share that with folks that don't have access to the workspace which is exciting um i can certainly also make sure i have pdf subscriptions i can take a look at the lineage i'm pulling that up here so we've got that connected uh you can kind of see what i'm highlighting and then of course i can cross filter so i'm going ahead and doing my favorite point and click here uh which is fantastic but this is not where the demo ends right the exciting thing about aibi is that we're not done just at the v the data vis layer we're going to go ahead and launch genie and ask new questions instead of asking an analyst so let's do how's my pipeline how's my pipeline ken's favorite favorite question i'll leave that part out and yep looks like we've got it by region love to see that perfect um let's slice and dice uh americas maybe let's tap americas and let's maybe look at it by segment perfect so we've got our segment here and let's go ahead and do a pie chart cool so while genie's getting that pie chart going for us let's take a look at the code so the code here is actually going to be exactly what we saw in the dashboard side of the house we're filtering out these forecast categories are null we've got demo stage validation stage procurement stage and we are again ingesting all that context from the fact that it's in our our pipelines and such perfect so let's go ahead and ask a new question let's ask something that was not in the dashboard let's ask about sales reps maybe who's our top rep let's take a look at that who created the most pipeline perfect great so let's see who's doing the best here lauren okay snaps for lauren um how does lauren compare to our average reps so compare to our average rep perfect awesome so let's go ahead and take a look we confirmed we're getting kind of some different numbers there but you know ken promised us we couldn't just get through messy data he promised us we couldn't just get through specifics to the business he told us we could plow through missing semantics so let's ask genie something it doesn't know let's go ahead and ask about churn perhaps okay great so genie did not hallucinate we love to see that um it's asking us to tell it more and instead of me having to go find someone with permission to update the semantic layer i'm just going to tell it what churn is i know what it is is so let's go ahead and do that turn is when we lose an op and don't win another after that perfect let's see genie can you learn this perfect okay so we've got some churned opportunities right here and you know the nice thing i'm going to do now is i'm going to hit this save as instruction so when i save it as an instruction that's going to let everybody else in the company leverage that same definition you know i have a colleague in europe they want to come in they want to ask the same thing they are going to be able to um and if they need to modify that definition they can so let's be that colleague in europe up now let's go ahead and who [music] churned last quarter in europe perfect so it looks like the we're seeing a list of those churn customers again genie also connected the dots between amia and europe so we're going to go ahead and say that's a w but let's actually see if genie can extend this knowledge let's ask it to calculate a churn rate so can genie calculate a churn rate let's say again in europe we're there we stay there mediterranean it's summer um and let's like really challenge it what was turn rate last [music] quarter there awesome okay so just like that jeie was able to learn what churn is from me and apply it let's just toss this in a little viz real quick make sure we have a nice kind of chart of where everything looks like turn rate by region awesome looks like america is doing pretty well on a low turn rate um but this is great right we were able to see how we went from a dashboard to asking follow-up questions that were not in the context of that dashboard and we were actually able to teach genie some missing semantic information have it then apply that in multiple contexts for multiple users and for genie to extend that understanding to new data so with that i'm going to hand it back over to mr ken and we are so excited to see what you do with [applause] aibi awesome what do you guys think all right now as i said up top anyone can make slides or throw together a demo the hard part is really making it work on real world data doing real world analysis which is why we've made a point to start working with customers on genie and a and aibi from the very beginning customers like accolade sega kythera t-mobile um and but my favorite feedback actually came from a conversation between one of our s and brian fox he's the cto of sona type he had lots and lots of different feedback for us but this is how he read wrapped up the conversation so i'll i'll let you read it i won't repeat it here but this type of feedback is really why we're super excited about the future that's ahead of uh for aibi but the truth is this is going to be a long journey and we're really just getting started and we really truly believe in this idea that in order to solve this problem for real we really need to work with real world data on real world problems with all of you which is why i am very very happy to announce that aibi is available to all Databricks sql customers today so you can start immediately building your dashboards uh just those are fully generally available and you can also just toggle on genie in your workspace uh right now our goal really is to make it possible for as many people to be able to use a aii that's our mission and along the same lines we're we're equally excited about the possibilities of opening up our a ai a apis wellow that's a that's a mouthful um opening that up for all of our partners so that they can benefit from all the work we've done as well we we believe in partnering and creating an open ecosystem so that everyone can truly democratize data and ai no matter what tools and experience they use thank you [music] [applause] [music] awesome super excited about genie and aibi okay please try it out that's really sort of the pinnacle of data intelligence for us to be able to really talk to your data we're going to make it better and better better give us feedback and we're open up the apis as we said so that our bi partners can also leverages so you can get the same kind of intelligence in your favorite bi tool okay so this is great i'm just going to wrap it up just give me one minute here um so what have we done today we've you see this is the data intelligence platform you can see it on the slide here so we've covered mosaic ai the generative ai stuff we've covered databas sql r did that aibi we just heard that and um tomorrow we have awesome talks let me just summarize for you a little bit what we saw just today so we saw on the tab side the acquisition uh how we're going to bring interoperability on these formats on generative ai how we can build your own ai and build it into production on your own custom data data warehousing we saw amazing performance okay and especially on bi so give it a try it's getting better every year and then of course aibi where you can just talk in english and tomorrow uh we're going to focus on workflows how to do data engineering and how we're going to actually run streaming pipelines as well as unity catalog we're going to hear from matay and the open sourcing and then of course delta link and we're going to have actually ryan blue from pache iceberg on stage here okay so that's it for today please go grab your lunch and be back here at 12:30 this is a super awesome talk on gen that uh handling tank is going to give the cto from mosaic so you know runs all of the mosaic ai stuff for us today um and we're going to also hear about dspi which is an open source framework that's super cool okay thanks [applause] everybody data is big but its potential is even bigger data intelligence holds the promise of curing diseases saving lives reversing climate change and changing the way we live we believe the future depends on data and unlocking its limitless potential with ai data grips is the data and ai company we help companies take control of their data put it to work with ai and solve the world's toughest problems because the challenges we face as businesses as people and as a planet aren't easy they can't be solved in silos they are too important to be left to the few we need all the data all the ai all the brain power we need all hands on deck in one place on the new data intelligence platform the only platform that brings ai to your data so you can bring ai to the world and that changes everything it expands our sense of what's possible it makes things simple it turns reactive into proactive so you can innovate faster because that's how innovation should happen collaborative and fast so let's defy assumptions break the mold map every genome cure every cancer watch the cosmos black every voice every vote take more moon shots and land them the heavens have become a part of from now on nothing stands between you and your data you and the answers because the power of data intelligence is the power of knowing now you know [music] [applause] everybody hey super excited day two here um we have an awesome program in front of us but i want to start by first again thanking our partners uh without them this program would not be possible so i want to thank the gsis and the hyperscalers and all the isvs that you see on this picture please go to the expo hall check out what they're up to okay so we have a really awesome program in front of us today uh you're going to hear from uh texas rangers uh you're going to hear from duck tb creator you're going to hear from mat zarya who started the spark project he's going to talk about uc we're going to hear about uh apache iceberg from the original creator of the project ryan blue we're gna and then we're going to hear from deposit and our studio tarf and then professor yin from udub okay and then we have lots of lots of announcements today uh before i jump in i wanted to quickly recap yesterday okay so in case you missed it yesterday we talked about the acquisition of taber which was a company started by the original creators of pache iceberg and what we talked about is how we intend to bring these formats closer and closer together delta lake and apache iceberg uh and if you want compatibility today or if you want interoperability today uh we announced the ga of uniform so store your data in project uniform stands for universal format and you will get the best of both of those formats okay we got all the original creators of both of those projects we're making sure that uniform really works well with both so that was the first thing that we announced second we talked about gen and in geni we kind of talked about how lots of companies are focused on general intelligence which is super cool models that are really good at anything you can ask them about history math and so on but we're focus on data intelligence data intelligence is not just general uh general intelligence it's intelligence on your data on your custom data on the proprietary data of your organization being able to do that uh at a reasonable cost and with privacy intact okay we talked about compound ai systems and the agent framework that we released yesterday that lets you build your own compound ai systems okay and then we heard from rold yesterday about data warehousing and he talked about the performance improvements that we've seen over the just last two years so just bi workloads concurrent workloads on bi we saw 73% improvement on the bi workloads that are running on Databricks the last two years okay so we're just tracking those over two years it's massive improvement so check it out and then i was very excited about aibi so aibi was a project that we built from the ground up with generative uh ai in mind to completely disrupt how we do bi today so that's also available in datab brakes so check it out okay so that's what we did yesterday those were the launches that we had yesterday uh but today let me see here my my clicker is not working how do we get to the next slide maybe one more time like that nope maybe it's out of battery okay can someone go to the next slide please no okay we're just going to talk about this slide today all right all right something is happening and now i'm worried about the next speaker um okay i will introduce the next speaker uh so next speaker um her name is y chin she's professor at udub she's going to be talking about slms okay what's slms everybody's talking about large language models these are small language models okay how do they work what makes them tick what's the secret sauce to make slms work really really well super excited to welcome on stage yin [music] he all right so i'm here to share with you impossible possibilities so last year when sam alman was asked how can indian startups create foundation models for india he said don't bother it's hopeless whoa first of all i hope that indian startups didn't give up and will not give up second of all this conversation could have happened anywhere else in us any universities or startups or research institute without that much of compute so here comes impossible distillation how to cook your small language models in an environmentally friendly manner and it tastes as good as the real thing so currently what we hear as the winning recipe is extreme scale pre-training followed by extreme scale post trining such as rhf what if i told you i'm going to start with gpt2 that small lowquality model that nobody talks about and somehow i don't know why how but somehow we're going to create or squeeze out high quality small model and then compete against much stronger model that maybe two orders of magnitude larger now this should really sound impossible especially when you might have heard of a paper like this that says the false promise of imitating propriatary large language models although what they report is true for that particular evaluation experimental setup they reported please do not generalize overgeneralize to conclude that all the small language models are completely out of league because there are numerous other counter example that demonstrate that task specific symbolic knowledge distillation can work across many different tasks and domains some of which are from my own lab today though let me just focus on one task which is going to be about how to learn to abstract uh in language to simplify this task let's begin with the sentence summarization as our first mission inoss so here the goal is to achieve this without extreme scale pre-training without rhf at scale and also without supervised data sets at scale because these things are not always necessarily available but wait a minute we have to use usually all three at least some of them but how are we supposed to do any good against larger model without any of this so the key intuition is that current ai is as good as the data that it was trained on we have to have some advantage we cannot have zero advantage so that advantage is going to come from data by the way we have to synthesize data because if it already exist somewhere on the internet open ai has already cwed it that's not your advantage they have it too so you have to create something genuinely novel that's even better than what's out there so usually the stallation starts with large model but we're going to tos that out just to show you how we may be blinded to the hidden possibilities so i'm going to start just for demonstration purposes with gpt2 that poor lowquality model and then i'm going to do some innovations which i'm going to sketch in a bit to make high quality data set that can then be used to train small model that will become powerful model for a particular task uh the only problem though is that gpt2 doesn't even understand your prompt it cannot do prompt engineering using gpt2 you ask it to summarize your sentence uh it generates some output that does not make any sense so then you try again because there's usually random to it you can sample many different examples like hundreds of examples and we find that it's almost always no good like less than 0.1% good where there's a wheel there can be what way so we had multiple sequence of uh different ideas that included our neurologic decoding this is plug and play inference time algorithm that can incorporate any logical constraints to your language model output for any of shelf models uh we can plug and play this to guide the semantic space of the output but because gpt2 is so bad even with this you know the success ratio was only about 1% but this is not zero now we are going somewhere because if you over generate a lot of samples and then filter out uh you can actually gain some good examples this way and then students brilliant students came up with many different ideas i'll close over the technical details but we found some ways to increase the chance success ratio to beyond the 10% just so that it's a little bit uh easier to find good examples so then overall framework goes to something like this you start with the poor teacher model you over generate a lot of data points and then because there's a lot of noise in your data you have to do serious filteration so here we used the three layer filteration system doesn't the details are not very important but let me highlight the first one intail mon to filter which was based on off the shelf inail mon classifier that can tell you whether a summary is logically entailed from the original text or not this is of the shelf model that's not perfect it's maybe about 70 to 80% good but it's good enough when you use this aggressively to filter out your data then weuse use that mo uh data to train smaller model much smaller model which can then become the teacher model for the next generation students so we repeat this couple times to make in the end high quality dim some data and high quality model when we evaluated this uh against gpt3 which was the best model of that time so this actually was done before chat gpt came out and we were able to beat that gpt3 which was uh at that time the best summarization model out there but since chpt came out you know people are like whatever you know chat pt can do everything including including summarization so why should we bother so here comes mission impossible to where we are now going to compete against chat gpt 3.5 and to make the challenge even harder for us now we are going to summarize documents not just the sentences and then we are also going to do all of these above without relying on that off the shelf in tont classifier i mean in practice you can do that just like academically we wanted to see how much can we push the boundary against the commonly held assumption about the scale so our new work in for sum is a information theoretic distillation method where the key idea is instead of that offthe shelf inment filteration system we're going to use use some equations that equation has actually only three lines of some conditional probability scores so that you can compute using ofto shelf language models um it's only 2 early in the morning so let's not drill into the details of the equations but i can just tell you hand w that if you shuffle this around you can interpret this as special cases of pointwise mutual information which you canuse use for the purpose of uh filtering your data so we use the same overall framework as before uh we now use the pia 2.8 billion parameter model because we liked it a little bit better than gpt2 and for the filteration we're now using the three short equations that i showed you earlier um and then uh we do the same business uh this time though we make the model even smaller only .5 billion parameter model uh that leads to high quality summarization data set as well as model so how well do we do well as promised we do either as good as chach pt 3.5 at least for this task or we do better depending on how you set up the evaluation challenges and benchmarks so you can check out more details in our paper to summarize i demonstrated how we can learn to summarize documents months even without you know relying on extreme scale pre-trained data uh pre-trained models and many other things at scale the real research question underlying these two papers though is this idea about how we can learn to abstract because uh right now the recipes let's just make models super big the bigger the better but humans you and i cannot really remember all the context you know like i don't know like a million tokens nobody can remember million tokens in your context you just abstract away everything i told you instantaneously but you still remember what i just said so far that's really amazing human intelligence that we don't yet know how to build efficiently through ai models and i i believe that's possible we're just not trying hard enough because we're blinded by just the magic of a scale okay so finally uh infin as the third mission impossible so switching the topic a little bit now the mission is to make classical statistical engram language models somehow relevant to neural language models how many of you even talk about angram models anymore i don't know do you even learn this these days um here we're going to make n equals infinity we're going to compute this over trillions of tokens and the response time should be super instantaneous and we're not even going to use a single gpu on this like wow let me tell you how hard it is so hypothetically if you're going to index five trillion tokens in a classical engram language model with n equals infinity then you're roughly speaking looking at two quadrillions of unique engram sequences that you somehow enumerate sort count and store some air which might take maybe 32 terabytes of discus space maybe more who knows but it's too much we cannot do that and if you look at what other larg scale classical engram models other people have ever built it was google in 20 2007 due to jeff dean and others who only scanned two trillion tokens i mean it was a lot back then um up to five engrams five engrams which already give them uh about 300 billions of unique engram sequences that they have to enumerate to sort count etc so it's too many people didn't do beyond that very much so how on the earth is it possible that we can actually blow this up to infinity so before i reveal what we did i invite you to go check out this online demo if you so desire in infin g.i /do infinity dg. /do so you can look up any token you want here's one example highlighted which has 48 characters i don't know why that word even exists but not only it exist if you look it up there are like more than 3 thousands of instances and it shows you what millisecond it took it's a 5.5 millisecond it took and then it also shows you um how you can tokenize that long word you can also uh try multiple words to see which word comes next so for example actions speak louder than what so it's going to show you on the web uh what are the other next word that comes and you know again it's just super fast so what did we do uh it'll be you'll be surprised to hear how simple this idea actually is there's something called the suffix array that i think not all algorithm classes teach but you some do uh it's that data structure that's implemented very carefully uh so we index the entire web corpus using this suffix array and the truth is we don't precompute any of these engram statistics we just have this data structure ready to go and when you call particular query we uh compute this on the fly but thanks to the data structure we can do this super fast especially when you do c+ plus implementation i know people don't usually use that language anymore when it comes to ai research but it's a good stuff that actually runs much faster how cheap is this so it's only a few hundreds of dollars we spent for indexing the entire thing and then even for servicing the apis uh you can get away with a pretty low cost and it's really really fast even without gpus the latency for different types of api calls are just a few tens of milliseconds you can do a lot of things with this um so one thing i can share with you right now is you can interpolate between your neural language models with our infin to lower the perplexity which is the metric people often use for uh evaluating the quality of your language model across the board so and this is only the tip of the iceberg that i uh expect to see i'm actually working on other stuff that i wish i could share but i cannot yet but we started serving this api end point uh a few weeks ago and already we served 60 million api calls not counting our own access so i'm really curious what people are doing with our infin so concluding remarks the tldr of my talk is that ai at least in the current form is as good as the data that it was trained on so the past 10 current ai usually depends prim primarily on human generated data but it could really be that the future will rely on ai synthesized data i know that you know there's a lot of concerns about this that oh maybe the quality is not very good there may be bias so you cannot do this in a vanilla way you should do it in a more innovative way but there are many evidences piling up that this actually works so segment anything by meta sam is an example of ai synthesized annotation on uh uh image segmentations uh helped with human validation but human alone couldn't uh annotate that many examples of images here's another example textbooks are all un need by microsoft i one to3 um again this is a case where when you have really high quality data textbook quality data synthesized you can actually compete against larger counterpart across many many different tasks maybe it's not as a still general as larger models in some capacities but this is amazing to serve a lot of business needs where you may not need generalist you may need a specialist and also what textbook alludes to you is that quality is what matters it's not just a brute force quantity but it's quality d 3 is yet another example why is it better than d to out of a sudden well in large part because of better captions but which better captions the previous model used all the good captions well they synthesize the captions that's how you get high quality data of course you have to do this with care but there are many more piling examples of uh task specific symbolic knowy distillation including uh the work of my own lab that demonstrate that this could really uh make smaller models really unlock the hidden capabilities of small models so it's really about quality novelty and diversity of your data not just the quantity and i'll end my talk here thank [music] [music] you awesome okay so there we had it mission impossible so the the secret sauce behind these slms small language models uh is the data surprise uh okay awesome so uh back to this slide we saw it yesterday so this is the data intelligence platform and this is sort of guiding us different portions of the platform that we're going through we went through a bunch of them yesterday and today the next level that we're going to go through is delta lake and uniform so we have a talk on delta lake um that was our agenda a month ago when we put this together but uh it turned out that uh you know we now have acquired company taber so we really really wanted you to hear from ryan blue the original creator of apache iceberg so i want to welcome him on stage and bring him on [applause] [music] [applause] [music] here hey ryan hey good to be here awesome okay so congratulations welcome to dat breaks thank you um we are really excited to be here and uh also excited to get started on this new chapter in data formats awesome so um what's the main benefit of joining Databricks why join forces i you know i've i've never wanted people to worry about formats uh formats have always been a way for us to take on more responsibility as a platform and take responsibilities from people who you know worry about things it when we started this people were worrying about whether or not things completed atomically and so this next chapter is really about how do we remove the the choice and the need to stress over you know am i making this uh the right choice for the next 10 years um that weighs a lot on people and i think we just we we want to make sure that everything is compatible um that we're all you know running in the same direction with with the the single standard if possible um hopefully we can get there yeah i think we're going to get there actually you you had a talk right a while ago that said something like i want you to not know about you know these formats in iceberg was was some title right something exactly i i don't want anyone thinking about uh you know table formats or file formats or anything like that that's a a massive distraction from what people actually want to get done in their jobs uh so i want people focusing on getting value out of their data and not the the minutia that's the the kind of nerdy problem that you know i get excited about leave leave that to us hey i like it as a nerd i think it's awesome we got thousands of people to learn how to do asset transactions and understand all the underpinnings of the stuff that otherwise would not give a damn about okay well everybody wants to hear like origin stories so can you tell us a little bit how did iceberg get started what's the sort of history well at netflix we were um really grappling with a number of different problem areas uh atomicity was one that we didn't trust transactions and what was happening to our data um we also had issues like you know more correctness problems you couldn't rename a column properly and those sorts of things and we realized that the nexus of all of the user problems was the the format level we just had too simplistic of a format with the the hive format and we decided to you know do something about it um and and then i i think the the real uh you know turning point was actually when we we open sourced it and started working with the community because it turns out everyone had that problem and we could just move so much faster with the the community it's been an amazing experience and you you started you were involved in the starting of the parket project before that right was it was was some of these thoughts even discussed to do this kind of atomist and so on back there or no this is so part of the my experience in the parquet project informed what we did here because there were several things that just were not file level issues they were this next level of you know really table level concerns like what's the the uh current schema of a table you can't tell that from just looking at all the files yeah um you know a lot of people think that this is uh you know the first time we're talking about these things you know you and i and others uh but this isn't the first time we're actually talking about you know interoperability and how to make this work right that's true um you know we've been in touch over the years uh you know talking about this several times um i i'm glad that we finally got to the point where it made sense um you know i think we were always going and and doing our own things but now we've gotten to the point where both formats are good enough that we're we're actually duplicating effort and the the most uh logical thing to do is this it is to start working together start uh you know avoiding any duplication if if possible between the two yeah that's super awesome okay so i think a lot of people here are wondering what does this mean for the apache iceberg community well i'm really excited because i i see this as a a big commitment and a pretty massive investment in uh the iceberg community and the health of both delta lake and iceberg in in general um i'm very excited you know personally to like work on this and and do a whole bunch of uh fun engineering problems um and and that'll be uh really nice awesome man super super excited to partner with you you know collaborate on you know delta uniform iceberg all these formats and then you know make it such that no one here ever needs to care about this ever again thanks so much thank you thank [applause] [music] you okay so now as i said originally this talk was just going to be about delta so now i want to welcome to stage the c of dat housing at dat breaks sean hpon to talk about delta and uniform [applause] [music] welcome thanks olly um and a lot of us we we used to work together with ryan in the past and it's really exciting to have him here so we could work together again um so this talk's going to be very exciting delta lake um first of all can announce the general availability of delta lake uniform what is what is uniform um really it's just short for two words universal format it's our approach to allow full lakehouse format interoperability see with all of these different formats delta iceberg hoodie it's essentially a collection of data files in par and a little bit of metadata on the side all of the formats use the same mvcc transactional techniques to keep that together and so we thought to ourselves in this age of llms transforming language left and right couldn't we just find a way to translate that metadata into different formats so that you just need to have one copy and that's exactly what we're doing with uniform the uniform ga allows you to essentially write data as delta and be able to read it as iceberg hoodie um and we've worked very closely with the pache x table and hoodie team to make that possible and we're going to be working with the iceberg team to make that even better the great thing about uniform is there's barely a noticeable performance overhead it's super fast you get awesome features like liquid clustering there's support for all of the different data types from map lists arrays and best of all it's got a production ready catalog with uc and uniform it's one of the only live implementations of the use of the iceberg rest catalog api and that's available for everybody using uniform there have been over over four exabytes of data that have already been loaded through uniform we have hundreds of customers using it and one of them in particular m science as you can see here was very happy that they were able to have one copy of their data which allowed them to reduce costs and have better time to value and it's innovations like uniform that are really making delta lake the most adopted open lakehouse format there have been over nine exabytes of data processed on delta yesterday over a billion clusters per year using it and this is tremendous growth it's 2x more than last year and if you're like me and when you saw these numbers i i did not believe 9 exabytes i literally up till yesterday we're going back looking at the code making sure they calculated correctly because it's just a tremendous amount of data every day that's going into delta um and it's adopted by a large percentage of the fortune 500 10,000 plus companies in production lots of new features but most interestingly it's sort of that last number they're over 500 contributors and best of all according to the linux foundation and this is their project analytics site it's open anyone can go to it today over about 66% of contributions to delta come from companies outside of Databricks and it's this community that just really makes us super excited and enables a ton of these features that are now available right and these are time- tested awesome innovative functionality things like change data feed law compaction i i love the row ids feature that just came out um but there are things like deletion vectors right deletion vectors are a way that allow you to do fast updates and dml to your data in many cases it's 10 times faster than merge used to be so if you have u dbt workloads or you're doing lots of operational changes to data delion vectors make your life easier and there have been over 100 trillion row rewrites that have been saved because of these deletion vector features and it's enabled by default for all Databricks users and so it's through these features that we've also been able to unlock access to this amazing ecosystem of tools that support delta and with uniform that's now ga we're able to get the same access to the hoodie and iceberg ecosystem so if you have tools and fun um sdks applications that work and there they're all part of the delta family now thanks to uniform and there's been some great improvements to a lot of the connectors the trino rust connector lots of awesome innovation happening here and a lot of that is thanks to this new thing that we've developed called delta kernel essentially at the core of all of this there's a small library that you can just plug and play into your applications or sdks that contains all the logic for the delta formats uh all the version changes new features and it's making it so much easier for people to integrate and adopt delta and most importantly stay up toate with the latest features and we've been seeing this the delta rust connector is community supported and has amazing traction uh just a few weeks ago at google's io conference i believe they bigquery introduced complete support for delta and very recently uh duck db added full support for delta and best part of this is we have hanis here who's the co-founder um one of the co-creators of duck dv ceo of duck dv labs professor of computer science who's going to talk to us a little bit about how they integrated delta into dctb hest get over [applause] [music] here hey thank you so much um yes hello and uh very good morning uh it's wonderful to see all of you here uh i have to adjust my eyes a bit to the amount of people um as uh shanta said i'm one of the people behind duct so for those of you who do not know what is duct tobe it's a small impress analytical data management system speak sql has zero dependencies and it's um yeah i'm having a lot of fun working on it with a growing team um and last year i talked about duct deb on this very stage for the first time and it was very exciting but also lots of things have happened since dan and duct land um there's been an incredible growth in adoption um for duct b we seeing all sorts of crazy things and here as an example um it's just the stars on github have doubled within a year to almost 20,000 and in fact we're so close to 20,000 so if you want to like it today then you know maybe we'll beit it but what also happened and just last week we actually released acb 1.0 and that was a big moment for us uh it was the culmination of six years of r&d in data management systems and what does what does 1.0 mean um it means that we have now a stable sql dialect and uh various apis and most importantly our storage format for dub is going to be backwards compatible from now on out um but maybe taking a little bit of step back how does stct fit into the general ecosystem um if we look at the world's most widely used data tools excel um and we look at very capable system like spark there's still a pretty big gap there's a lot of data sets that are not going to work in excel but they are maybe a bit too small to actually throw sparket them so dctb is really perfect for this last mile last mile of data analysis where you may not need a whole data center to compute something um so for example you have already gone through your log files in spark and now it's time to do like some last mile analysis with dctb doing some plots what have you that's where dctb fits into this big picture but now we have to somehow get the data between spark from spark to dub so how are we going to do that obviously we're going to use the best tool for the job available right csv f maybe not um so typically people use paret files for this uh obviously both spark and dub can read and write parket files so that works really well but we've all heard about the issues that have appeared with updates and schema evolution these kind of things which is why we have lakehouse formats so today we are announcing uh official duck db support for delta lake um it's going to be available completely out of the box with zero configuration or anything like that um but this we have done a bunch of these integrations and one thing that's really special about the delta lake integration is that we use this delta kernel that Databricks is building with the community and that's really exciting because it means that we don't have to build this from scratch like we used to for example with the paret uh reader but we can actually delegate a lot of the hard work of reading delta files to the delta kernel while at the same time um keeping our you know operators within the engine and so on and so forth so it's really exciting um we also made an extension to for dub uh that can talk to the unity catalog so with this extension we can find the delta leg tables in the catalog and then actually interact with them from db itself so here we can see a script that actually works if you install duct tob now um you can install this unity catalog extension you can create your secret which is like credentials and then you can basically just read these tables as if they were local tables um if you want to hear more about this there's actually going to be a talk this afternoon at uh 1:40 just look for dub in the title um so the delta extension joins this growing list of dub extensions um for example example there's others for iceberg vector search spatial and this sorts of thing but as an open source project and a small team we really excited about tabular um and Databricks beinging delta lake and iceberg closer together because for us it means we don't have to maintain to so different things for the same essentially problem and we're really excited about that means less work for us and i think everyone wins i just want to plug one sort of small thing that we're actually launching today um i've mentioned extensions to dub we've seen a lot of uptake in duct db extensions um and from now on actually we are launching community extensions which means that everyone can make dctv extensions and basically publish them and then installing them as is as easy as just typing install into a duct be near you so that's all for today um thank you very much and i will give back to [applause] shant that integration is super awesome it's very exciting okay so how do we top that by not going back by going forward and forward to delta 4.0 so we just we have the branch cut it's available delta 4.0 is the biggest change in delta since history it's jam-packed with new features and functionality um things like coordinated commits cations all sorts of new functionality that make it easier to work with various different types of data sets uh we won't have time to go through all of this so i'm going to pick a couple and dive into why these are such amazing features so liquid clustering is generally available now as part of delta 4.0 and with liquid clustering we really wanted to set out to solve this challenge that so many people have brought up partitioning it's good for performance but it's so complicated you get overp partitioning small files you pick the wrong thing it's a pain to resolve and liquid solves this with a novel data layout strategy that's so easy to use that we hope all of you will say goodbye to partitioned by you never need to say that again when you define a table not only is it easy to use we found out is up to seven times faster for rights and 12x faster for reads so the performance benefits are amazing and of course it's easy to evolve the schema make changes define anything without having to worry about all your data being rewritten in transforms and you know there about 1,500 people customers actively using this the adoption has been insane uh over 1.2 zettabytes of data have been skipped and you don't have to take my word for it even shell when they started using it for their time series workloads saw over an order of magnitude improvement and performance and it was just so easy to use next open variant data type and this one's really important that first word open is really exciting so what happens is now in this world of ai you have more and more semi-structured text data um alternative data sources all of this coming into the lakehouse and we wanted to come up with a way to make it easier for people to store and work with these types of data in delta um and usually what happens is when you're stuck with semi-structured data most of the data engineers they sort of have to make a compromise and you know none of us like to make compromises but usually it's about being open flexible or fast and often they'd only be able to pick two out of these three so for example for semi-structured data one approaches just store everything as a string right that's open it gives you tons of flexibility but parsing strings is slow right why why would you store a number as a string and have to reread it every time so of course there's an option to pick the fields out of your semi-structured data make them concrete types and you get amazing performance right this is open very fasty access however if you have sparse data you sort of lose out on a lot of that flexibility to modify the schema and you know relational databases for a while have had special enum or variant data types but all of those had always been proprietary if you wanted to use them to get a balance of not having to store everything as a string and not having to shard out every single column you got locked in and so that's why we're very excited with variant to be able to kind of get that sweet spot in the middle you can have your json data store it with flexibility fully open with amazing performance right it's very easy to use it works with even you know complex json here's an example of the syntax and we found of course it's eight times faster than storing your json data as raw springs this is just tremendous so if you're storing json in a string field today um go back to work or home and start using variant it's available in uh dbr 153 but most importantly all of the code for variant is already checked in to apache spark there's a common subdirectory in the 40 branch right now that has all of the implementation details for variant and all of the operators and there's a binary format code definition and library that we've made available open source so all the other data engines can also use variants we really want this to be an official open format that everyone adopts so that finally we have a non-proprietary way of storing semi-structured data reliably and so with that yeah oh it's a big deal with that i just want to summarize delta lake 4.0 it's interoperable we have this amazing ecosystem with people like hanis working together making it better and stronger you get amazing performance benefits and all of this is just so much easier to use now than it ever was before thank [music] [applause] [music] you awesome all right okay so back to the sta intelligence platform site so that was awesome we heard about delta uniform we from ryan blue uh isn't it cool that du db now will support riding natively into delta and uniform and uc super cool okay uh and then we have delta 4.0 so that's awesome all right so next uh creator of or original creator of uh apache spark project mat zahar is going to tell us about unity catalog okay so we have a lot of announcements here and this is a long talk so it's going to be super super exciting to see so let's welcome m and also pay attention to his t-shirt and his shoes carefully okay welcome m to stage [music] yeah all right hi everyone uh thanks ali yes i have the new unity catalog t-shirt um y you'll be able to get one soon i think somewhere so um all right so i have a you know somewhat longer session for you today because i'm talking about um uh governance with open source unity catalog as well as data sharing if you're familiar we have another open source project we launch delta sharing that's really making waves in the open uh data collaboration space um and um and we have a lot of exciting announcements around that so i'll start by talking about what's new in unity catalog and what it means to open source it why we did it what's in there um and then uh so ali announced that we're open sourcing it yesterday but he led me keep one more thing to announce today that that i'll talk about that's the next uh big direction for unity c uh and then finally i'll switch gears to sharing and collaboration and we'll have some cool demos of all these things too so let's start with unity catalog so i think everyone who works in in data and ai knows that uh governance whether it's for security or qualityy or quality or compliance remains a huge challenge for many applications and it's getting even harder with generative ai there are new um regulations being written all the time about it i heard in california alone there are 80 builds proposed that regulate ai in some form um and also you need to really understand where your data is coming from if you're going to create models and deploy them and run these applications so we hear things from our customers all the time about how they would love to use ai but they can't really govern it with uh you know their existing frameworks and even on the data space it's complex enough even there all the all the rules are changing and people are really worried about how to best do it so we wanted to to step back and um think you know it's 2024 if you had to design an ideal governance solution from from scratch today what would you want it to have you know we we asked a bunch of cios and we thought you you really wanted to have three things the first thing is what we call open connectivity so you should be able to take any data any source that's in your organization and plug it into the governance solution because no one's going to migrate everything into just one you know data system over time most organizations have hundreds or thousands of so you really want a governance solution that can really cover all this data wherever it lives in any format you know even even in um in in in other platforms then we also really think you need unified governance across data and ai i think it's clearer than ever with generative ai that you have to manage these together you know you can't be managing ai without knowing what data went into it and also all the output of ai you know as you as you do serving uh is going to be data about how your application is doing it's got the same problems of course quality and security so we really need it to be unified and then finally um we heard everyone asking for open access from any compute engine or client uh because there are so many great um you know solutions out there uh they'll keep coming out uh there'll be the next data processing engine the next machine learning library you want it to work with your data so this is what we're building uh with unity catalog especially with the open source launch today so first of all open connectivity unity catalog on database uh and Databricks as a platform uniquely lets you connect data in other systems as well and process it together in a very efficient way through this feature we call lake house federation so you can really connect all your data and give users a single place where you set security policies you manage quality and you make it available um it also is the only um governance system in in the industry really that's unified governance for data and ai so since the beginning since we launched this uh 3 years ago we had support for tables models files and we're adding new things as they uh new new concepts as they come out in the ai world like tools which we talked about yesterday with the tool catalog concept for gen agents and for all these things you can get these capabilities on top ranging from access control to lineage to uh monitoring and and discovery and finally um one of the the big things that is possible today through the open api and open source project we just launched is open access to all all your tables uh through a wide range of um applications i'll talk more about that in a bit uh but the cool thing here is again it's not not just data uh uh systems like dotb but also a lot of the leading machine learning ones like lang chain can integrate with unity catalog um and since we launched this uh it's it's been extremely widely adopted most of our customers use unity catalog now um and some of them are managing tens of pedabytes of data on it like gm or have thousands of uh active users z pepsi here so i'm going to briefly talk about some of the new things some quick announcements in each of these areas and then we're going to see a demo of how all these things fit together uh including with the open source project so let me start with open connectivity um so i'm really excited today that to announce ga of lakehouse federation the ability to to connect and manage um external data sources in unity yeah thanks everyone so this is yeah this is feature that we uh launched last year a year ago at summit uh it it builds on on apache sparks unique ability to combine many data sources efficiently and it lets you mount these data sources in Databricks set governance rules on top and get the same experience managing quality tracking lineage and so on uh as people work on them that you get with the you know with with your delta tables in there um and it's been going extremely quickly we now have 5,000 monthly active customers and if you look at the that graph of cze on lakeh house federation it's still growing exponentially um so we see a lot of customers that can finally bring all this disperate data which is a reality in every company you know as much as every data vendor would love you know for you to have all your stuff in in one system is just not the case uh and actually work with it together uh and another really cool thing that we're announcing is lak house federation for apache hive and glue so if you've got an existing hive meta store or glue you've got lots of tables in there you can now connect that efficiently to unity catalog and manage the data in that as well uh and that's rolling out later this year um so very excited about this i think it's a it's a defining feature of of of um databas as a data platform okay so what about unified governance across data and ai there's so so much happening in this space and uh our team has been working hard to launch a whole range of new features here so i i won't you know i won't even have time to go through all of them but uh just in the past year we've got everything from ai assisted documentation and and tagging to a lot improvements to lineage sharing uh monitoring um and so on um and i'm just going to to highlight two um two announcements here uh around abac and around lakehouse uh monitoring um so first lakehouse monitoring is going ga um so lakehouse monitoring is the ability to take any any table or machine learning model in unity catalog and automatically check for quality on it there's a lot of built-in quality checks like are there a lot of ns you know has it stopped being updated and so on plus you can do custom checks and the great thing about this since it's integrated into the data platform we know exactly when the data changes or when you know the model is called so it's very efficient and it does all this computation incrementally uh and it gives you these rich dashboards about quality you know uh uh classification of the data discovered and so on plus all these reports go into these tables so you can just quy you know programmatically see the quality of your entire data and ai um sort of estate there um so that's going ga today um and um sorry i should say that that's been uh that's already that's also already in use at thousands of customers uh and then the second thing that we're we're launching a preview of soon is attribute based access control so we've developed this policy builder where um and and tagging system where you can set tags on your data at any level in the catalog you can al will also autop populate tags based on um you know patterns of data we discover uh and you can um then then propagate you know the these masking policies across all of them and this works easily through the ui or through sql so these are just two of the things thanks yeah thanks and then the you know last but not least uh the thing i'm probably most excited about um is is the the the launch of open source unity catalog and open access so a lot of people asked me yesterday why why are you open sourcing unity catalog uh and really it's because um you know customers um want customers need it customers are looking to design their data platform architecture for the next few decades and they want to build on an open foundation and today even though you see a lot of cloud data platforms that claim you know support for openness in some way when you dig into them they don't really truly uh um you know um follow through on that so there are a lot of cloud data warehouses out there for example that could read tables say in delta lake or iceberg but most of them also have their native tables that are more efficient more optimized and they really nudge you to use those and to have your data locked in and then there are other platforms even some of the lake platforms where um you know it seems like hey everything's in an open format but you have to pay for always on compute and pay a high fee if you ever read it from outside inside their engine and customers don't want that uh everyone is saying they want an open lakehouse where they they own the data no vendor owns the data without locking and where they can use any compute engine in the future um so we've you know we we've been big fans of this approach for a while we think it's where the world is going so that's why we design uh everything we do to support it um and already today in Databricks you know all your data is in an open format there's no uh concept of a native table uh we also pioner neared this cross format compatibility last year with uniform where all these ecosystems of clients that only understand apache iceberg or hoodie can still read your tables and the next logical step is to also provide um open-source catalog um so this is what we have uh in the first release of unity catalog um we we'll be you know gradually releasing um uh the things that we built in our platform and and and um you know removing all the dependencies on on datab break code and putting them in the open source project but even in the for to release we you'll be able to govern tables unstructured data and ai um and we're really excited uh we proposed the project to the linux foundation and it's been accepted this morning so it will be there um yeah thanks and and then another uh cornerstone of unity catalog is we're doubling down on the cross format compatibility approach so even the first release uh implements the iceberg rest api it's it's one of the first you know open-source cataloges that uh that implements uh this this format so you can connect to it from any engine that understands that uh and we hope that means that a lot of the uh the ecosystem out there will work with it all right so you might be asking you know is this for real when are you actually releasing it maybe in 90 days maybe 89 days because a announced it yesterday um i'm just going to walk over to my laptop right here yeah so this is this is unity catalog on github you know looks looks solid to me people are working hard on it so just going to go into the settings here scroll down to the danger zone and make this thing public yep i want to make it public i understand make it public [music] all right and i think it's public now so yeah take a look so github.com unity catalog thanks everyone yeah so that wasn't that hard and of course we you know we invite all of you to to contribute we'll be working hard to expand the project and we want to do it all in the open we're not going to keep it uh you know closed for for a while uh to build this stuff up uh all right so yeah it is it is now available my slite is right um so we just released version 0.1 um it's this version supports tables volumes for unstructured data and ai tools and functions so it implements that tool catalog concept we talked about yesterday has the ice book support and if you look at our website it's going to it has open source server apis and clients and these work just as well with your instance of unity catalog on Databricks so everything built there you can just connect to your current data um we're also really excited to have a great array of launch partners uh everywhere from the um uh you know the the cloud uh vendors uh some of which have been contributing a lot to uh to open standards like the iceberg rest api already uh to uh leading companies in ai uh and in governance um so microsoft aws google uh they're all excited to see this happening and we hope to work closely with them uh to to um you know contribute to apache iceberg as well uh and to help define the standards for this so customers get the interoperability that they want um and then okay sorry and of course there is a lot more coming uh we're working on uh bringing a lot of the the the nice things you have in in Databricks and unity out here um including delta sharing models ml flow integration views and other things and we invite you again to to um to collaborate with us so that's kind of an overview of unity some of our launches in there it's great to hear about them but even better to see a demo and for that i'd like to invite zan papa one of our product managers to show you through all these new [applause] [music] features thanks mat i'm glad to be here i'll walk you through each of the features that m talked about let's start with the catalog explorer over here on the left hand side this offers a unified interface for browsing applying access controls and quering tables it enables you to nav uh navigate and organize cataloges tables functions models volumes and other data systems both within and outside of Databricks in some cases only some of your data will reside in the lake housee to address this we've simplified and secured access control to systems such as big query catalog such as glue and hive mysql postgress red shi shi snowflake and azure sql all of this is powered by lak house federation switching over here on the left hand side to a sql editor i'll show you how to query an external data system by running some sql that will join a store report table that is federated from snowflake along with a lakehouse native source that contains data on retail store returns once this table is created this store report table will become a unity catalog managed object which means the platform now handles all of your table management challenges including automatic data layout automatic performance and predictive optimizations for you but managed doesn't mean locked in this table or any unity catalog object is accessible outside of Databricks via unity catalogs open api let me show you how easy it is to query this newly created object using duck db first i will opt this table in for external access as i've done for other tables in this catalog next i'll switch over to duct db which is the same nightly bill that you can access right now i'll attach this catalog accounting prod to duck db and now i'll run a quick query to see all of the tables in this catalog all right and as you can see that store report table that i just created is right there next i'll run a quick query to select from this table i could do the same tables created in unity catalog and quickly query them using duck db's native delta reader this is data rick's commitment to open source and open interoperability here and [applause] now so far i've walked through unity uh unity catalogs explorer lak house federation and our new a open api however a major challenge for many organizations is ensuring consistent and scalable policy enforcement across a diverse range of data and ai assets let me show you how easy it is to scale your governance using tags and abac policies combined with proactive pii monitoring let's switch over here to the online sales prod catalog and let's take a look at this table called web logs one of the features that's been enabled in unity catalog is lakehouse monitoring which allows for simplified and proactive pii detection of anomalies in your data and models within this dashboard over here on the le hand side you can explore columns and rows and you can see that pi has been has been detected in the user input column now this is obviously a problem before this data set can be actually used this data must be mased and appropriate policies must be applied let's switch back to the catalog explorer back in this explorer over here in the rules tab a new rule can be created to express policy across all data it's now so much easier to mask all email columns across all tables of the single rule let's give this rule a name let's call it mask email all right and we're going to give it a quick description let's mask some emails and we want to apply this to all account users all right and this is a rule type of column mask and we're going to select a column mask function that i previously created conveniently called mask email and we want to match on a condition when a column is detected that has a tag with a tag value of pii email all right let's go ahead and create that rule and that's it and now to validate this mask we're going to go back to the web log sales table and we can observe here in this table in the sample data on the user input column over here to the right that the data has now been masked [applause] since this applies to the entire catalog let's go to a different table uh somewhere in here there we go as you can see we've got an email address column in here as well tagged pii email let's go up to sample data as you can see over here as well this table has also been masked with one rule so m i've shown how unity catalog enables organizations to have open access to their data seamlessly no matter where it resides well applying unified governance to ensure its integrity and security thank [applause] [music] you thanks thanks so much zan all right so that's uh that's unity catalog and action so as i said uh ali led me keep one thing to announce today uh so uh which i'm really excited about so what we just saw is you know you you could set up a catalog it's open it's got access control monitoring you can get to it from any engine you can federate stuff into it are you done you know as an engineer you might say this is pretty good what will happen unfortunately is you know someone will come in and ask a business question for example how is my ar trending in emia and to answer this kind of question there's not enough information in just the catalog in just the table schemas and things like that so you have to understand things like what is how is ar defined that that's some some kind of unique calculation for your business um maybe there are many tables that mention ar which one actually you know is the right one to use to get this information how is a mia defined you know which countries are really part of it and so on um and so the question is how do you bridge this gap um so this is something that is typically done in some kind of metric layer and we're really excited to announce that we're adding first class support for metrics as a concept in unity catalog so yeah yeah thanks yeah so this is this is something um we we've um u you know we will will be um holding out um uh uh later this year um so uh so unity catalog metrics so um the the idea here is that you can define metrics inside um unity catalog and manage them together with all the other assets so you can set governance rules on them uh you can find them in search you can get audit events you can get um lineage for them and so on and like the other parts of unity we're taking an open approach to this uh we want you to be able to use the metrix in any downstream tool so we're going to expose them to multiple bi tools so you can pick the bi tool of your choice uh we'll of course integrate them with aibi uh one of the things we're excited about is we're designing this from the beginning to be ai friendly so that aibi and similar tools can uh can really understand uh how to use these um and and and give you great results uh and you'll be able to just use them through sql through through table functions that you can compute on um and we are also partnering with uh dbt cube and ad scale as external metrix provider to make it easy to uh bring in and govern and manage metrics from those inside unity so to show this stuff in action i'd like to invite zishan to this stage again for a quick demo of metrics [music] thanks m as you mentioned metrics enhance business users ability to ask questions understand their organization's data and ultimately make better decisions rather than sifting through all of the data certified business metrics can be governed discovered and queried efficiently through unity catalog having already discussed the catalog explorer let's dive into business metrics in this overview tab here you'll see a list of all available sales metrics a few of these metrics are marked certified this indicates a higher level of trustworthiness we're going to select the web revenue metric down below here as you can see it's also marked certified by clicking into this you can see the metadata that's associated with this metric and the predefined dimensions that are associated with web revenue these are used when querying the metric such as date or location this is like having a built-in instruction manual for your data on the right hand side here you can see the metric overview section this is where this is where you can see the description of the metric who edited it and who certified it you can also see information about where the metric came from and where the metric is used such as notebooks dashboards and genie spaces let's click into this dashboard as you can see in this dashboard over here on the right hand side in the xaxis column i have all of the interesting information such as the dimensions and uh country city state etc this allows you to slice and dice without needing to fully understand the data model let's go into a notebook as well this metric isn't just usable in a dashboard it's also queriable from external tools and notebooks in this notebook we're using the get metric function to pull all aggregated data it's that simple finally let's go back into a genie space here here web revenue metrics can be used to answer natural language questions in this space you'll also see that this visualization was created by asking about the revenue generated across states using this metric this approach extends the reach of these metrics throughout the organization making them accessible to business users as you can see m unity catalog metrics make it easy for any user to discover and use trusted data to make better decisions [applause] [music] all right thanks desan super excited about this um so uh after doing two demos this morning i think zan can have the the rest of the day off very um very nice to see those um all right so the the final portion of the talk i want to talk about i want to talk about what we're doing in sharing and cross or collaboration um you if you know if depending what industry you're in you've probably seen that uh data sharing and collaboration between companies between organizations uh is becoming a really important part of the modern data space uh it can help you know providers and suppliers coordinate better it can help you know uh uh streamline a lot of business processes just yesterday i met a customer who thought that they could speed up basically launching new drugs by a factor of two uh by uh implementing these kind of technologies um so so really u you know really powerful way for for many industries to move forward um and we started looking at this area about three years ago we wanted to provide great support for it and we started by talking to a lot of data providers who collaborate and what they told us um was that uh many of the data platforms out there support some kind of sharing between different instances but it's always closed you can only share you know within that same data platform within that you know customers of that data warehouse or whatever and as a provider as you know any company that wants to collaborate with a lot of partners this is very restrictive so emper for example who's a who's a cdp um said that they would prefer to invest in open solutions u that uh let them you know set up data collaboration once and then be able to reach anyone regardless of what platform they're computing on so that's the approach that we've taken with all our sharing and collaboration infrastructure uh by creating an open collaboration ecosystem based on uh open standards and the core of that is delta sharing a feature of delta lake that allows you to securely share uh tables you know across clouds and across data platforms and then we've built on that with databas marketplace and datab break clean rooms um so if you're not familiar with delta sharing uh basically uh this is a a core part of the delta lake project where if you have um you know a table and increasingly other kinds of assets as well uh you can run this this server that has an open protocol uh and serve out just parts of your table to other parties that are um authorized to access them and because the protocol is open it's a very simple one based on uh parket um you know files that that are that that they're given access to uh it's really easy to implement a lot of consumers so of course you can use Databricks to access these but you can also just use pandas apache spark even bi products like tau and powerbi um are are letting you load data right into there um and it makes a lot of sense you know if you're a data provider you want to publish something why should the other party even need to install a data warehouse in the first place why not deliver that data say straight to tableau or straight to excel or something like that um so so that's delta shing uh it went ga two years ago and uh it's continuing to go extremely quickly uh so just this year uh i mean just now we have over 16,000 recipients that are receiving data through delta sharing uh from our customers on the briak platform and this is growing by a factor of four year on-ear so there's no no end inside we're super excited about this and the other thing i'm really proud of is that uh 40% of those recipients are not on Databricks so this idea of cross uh platform collaboration uh is is real and our customers are able to deliver uh data and to have you know real-time data exchange with uh anyone regardless of what data platform they're using so super excited about the growth of that this year um we are uh continuing to expand delta sharing and one really um exciting announcement is that we're hooking together two of the the best features of the platform lak house federation and sharing to let you share data automatically from other data sources as well so we we talk to a lot of companies who have some data and a data warehouse or they have a partner who you know isn't on Databricks has another uh platform but they want to collaborate and since we built this federation technology that can efficiently um query this data push down filters get it out uh and uh and deliver it um we are just connecting that to delta sharing to let you seamlessly do this so now you can really share data from you know any data warehouse any database with any app that understands the delta sharing protocol um so that's that's delta sharing thanks yeah excited about that feature yeah all right so one of the um the the things that builds on delta saying is datab break marketplace this is something uh we launched um about 2 years ago and uh it's um it's um uh also been going extremely quickly um databas marketplace has now up to over 2,000 listings again going more than 4x year on year um so super excited to see that um and this makes it actually up there with the largest uh data marketplaces anywhere in the cloud on on any platform so it's it and it's it's it's continuing to go uh our team has been adding a whole bunch of new functionality there that providers are asking for like private exchanges uh sharing of nonata assets uh like models and volumes usage analytics and even support for non- databased clients if you put data in there um you know you can you can reach these other platforms as well um and then we are also super excited to welcome uh uh 12 uh new uh partners to this um to to to the shing and marketplace ecosystems uh some of these announcements went out last last week uh but anywhere from from axium amperity atlassian uh industry leaders in in in many different domains are now connecting to this ecosystems and making data available to users on Databricks or really on any platform that implements the open sharing protocol and they join our existing ecosystem of partners so thanks to all of them um who are uh participating in this great so yeah really really excited to see how this will continue to go in the future um the final thing uh i want to talk about is that we're soon launching public preview of datab breaks clean rooms um clean rooms are uh a way to to to do a private computation with with another party where you can each bring in your assets you can bring in some tables some code some unstructured data some ai models any kind of asset you can have on the databas platform and you can agree on a computation with someone else that you r and then send the to just one recipient so for example it could be as simple as you each have some tables and you want to figure out you know how many records you both have in common and it can be as complicated as you know someone has a machine learning model they want to keep private someone has a data set they want to keep private but they want to apply these two together and get the predictions or get the differences between their two models um so two things really distinguish Databricks clean rooms from other clean room solutions out there the first is because you have this complete data and ai platform you can really on any computation it can be machine learning sql python r and so on versus just sql in in many other kleen solutions uh and also uh datab big clean rooms is built on delta sharing it integrates with lakehouse federation so it's very easy to do cross cloud and even crossplatform collaboration if someone's primary data store is not Databricks they can still seamlessly connect it to the klean room and uh do work on that uh so this is going to go in into public preview um uh just a little bit later summer um and we've already seen some really awesome use cases uh one uh company we've been working closely with uh is mastercard who has so so many exciting use cases with a whole range of different partners you can imagine uh you know the the different kinds of things that they they can do with their data and uh who is u uh you know looking for the best way to do uh private versions of the state-of the dart um um algorithms and techniques to work with this data um so i want to show you all this collaboration work in action and for that uh we have our third demo i'd like to invite darana who's our product manager for clean [music] homs thanks mate picture this i'm part of a media team at a large retailer we are teaming up with a supplier to run a joint advertisement campaign to to grow our sales for this we need to identify our target audience we need to collaborate on join customer data i as a retailer have data on my customers and their shopping behavior my supplier has their customer loyalty data however we have some challenges first we cannot share any sensitive information about our customers with each other second our data is on different clouds regions and data platforms and finally we want to leverage machine learning and python for our analysis and not just sql Databricks clean rooms can help with all of this in a privacy safe environment let's see how so here i am on the Databricks clean room console as the retailer and i create a clean room in a few simple steps i'm using azure and east us2 as my cloud and region and what's amazing is that it doesn't matter that my supplier and i are on different regions and clouds i then go ahead and specify my supplier as a collaborator and once the clean room is created i bring in the data i add my audience cft table and what's awesome is that i can also bring in unstructured data and ai models to collaborate with and here i go ahead and add an ad science private library that i've created that invokes an ai function to help me with my audience segmentation task so now it's time to add a notebook and i add one i've pre-created for audience segmentation that i've preconfigured to use my private library and the best part about this notebook i can use use python for machine learning now my clean room is ready for my supplier to come join so let me flip hats i'm now the supplier hence dark mode and i join the clean room that my retail counterpart added me to i see all the assets that they brought to the clean room and when i click into the audience graph table i see the metadata associated with the table but not the actual data this is perfect context for good collaboration while ensuring that i'm not privy to any sensitive information now it's my turn to bring in my customer data but my customer data is on a snowflake warehouse outside Databricks and i don't want to create a custom etl pipeline to bring this data in and i don't have to because lucky for me i can directly specify lake house federated tables as sources to this clean room with no copy no etl these clean rooms truly scale for crossplatform collaboration and now my favorite part i inspect the notebook the code looks good and i run it so the job run has successfully started and in a few seconds it's done and i'm presented with delightful visual results to help me understand that we can target 1.2 million households for our campaign based on factors such as customer age income bracket and household size thank you so let's go back to our slides to summarize what we just saw our retailer and supplier with able to bring their respective customer data to a privacy safe environment the clean room and collaborate without sharing any sensitive information with each other it didn't matter that they were in different clouds regions or data platforms they could collaborate on more than just structured data and they were able to use python for machine learning thank you all so much and back to you [applause] [music] mat awesome demo yeah so super excited about clean rooms uh and especially uh crossplatform clean rooms i think it's really going to transform a lot of industries it just makes sense to be able to collaborate on uh data and ai in real time in a secure fashion um so i think overall i've given you a good sense of our approach we really believe that uh picking you know the right uh uh governance and sharing foundation uh you know for your company is essential for the future and we think it it needs to be an open and cross platform approach we've been thrilled to see both unity catalog and delta sharing you know go from just an idea to being uh you know used you know virtually all our customers in a few years and uh we're excited that both of these are open um we're excited about the partners and we invite you to join the open ecosystem um so that's you know that was a lot of tech in the sknote but the exciting thing is what you can do with the tech and for that um i'm super thrilled to um invite um our next speaker uh an actual sports star for the first time on the data and summit stage uh alexander boo from the texas rangers and the texas rangers are one one away from their first world championship texas takes the lead it's happen the texas rangers win the world series champions in 2023 [music] that was a huge moment for us as a baseball organization moving from the bottom of the league to winning our first ever world series all credit must go to the players and coaches that made this happen this is also a huge moment for our community as over 500,000 people attended our world series parade and for me growing up a lifelong rangers fan it was a dream comeing true however this was also a win for the data team that i lead at the rangers and i'm here today to talk to you about how we use data intelligence to drive competitive advantage and transform how the modern game is played most of you may know this but baseball has always been a datadriven sport whether it's comparing statistics on the back of baseball cards to the modern age of moneyball however how data is used in decision making has changed dramatically in this modern age of ai data used to be descriptive evaluating past performance now data is predictive optimizing our understanding of future player performance one example of this is how we're using data and ai for biometric insights we build predictive models on how the body's motion affects how a ball is thrown leading to designed pitches guided by ai that are personalized for each unique pitcher further with a better understanding of how players move when swinging the bat we can provide biomed recommendations to optimize for specific types of hits with these insights we can advise our players you're trying to hit for power get those legs and try to get it out of the ballpark you want to just hit for contact just square up the ball in little league my coach would always tell me to choke up on the bat and bend your knees we now measure that at high frame rate 300 frames a second this pose tracking gives us further insight into injuries and workload management too data and ai helped make the most of our players athletic talents leading to those incredible clutch hits that maybe you all saw during the world series post tracking isn't our only new data source we also track every player's position continuously at 30 frames a second for every major league game this gives us unprecedented ways of measuring defensive capabilities by understanding tendencies reaction times and the way that our fielders move when trying to catch that fly ball we can optimize our defensive placement using ai to maximize the likelihood of a player making that out and yeah maybe we got a little bit too good at that and major league baseball changed the rules a couple years ago but we still use it to this day this culminated in a playoff run where we went 11 and0 on the road highlighted by impressive defensive plays such as home run robbing catches and clutch double plays how did we change this data and ai game i say it was it wasn't always like this getting to this point where we could realize these successes was not easy there were so many challenges rather that we faced just a few years ago when we began our data modernization journey stop me if any of this sounds familiar our on-prem stack could not scale to these new data sources those ris in it cost in the maintenance of these on-prem servers led to an untenable roi on ai investments further as our data team grew supporting minor league operations all around the country as well as scouting initiatives all around the world those governance and permissions became difficult to manage we lacked governance and ran into fragmented silos our data teams were split between minor league player development amateur analytics international and advanced scouting teams these slow and disjointed processing within silos led to delays of reports that our players and coaches needed in some cases we weren't delivering reports until the next day well after the game had already finished and while we don't have a live link to the dugout perhaps uh caused by a certain trash can banging incident a few years ago it is still imperative that our players receive that information post game to prepare themselves for what happened and how to be successful tomorrow with 162 games baseball is a marathon and quick feedback is a necessity for our players to solve these problems we have unified and simplified our data and ai stack on the lakehouse with Databricks unity catalog unites our data silos under one roof we have a variety of data with sensitive information such as player addresses financial contract information further biomechanical and medical records should not be widely accessible through the org unity catalog allows us to have that single- shared platform with appropriate permissions in place to comply comply with both internal and external regulations such as furpa and hippa unity catalog also gives us the ability to manage clusters etl pipelines all within the json metadata once our data is loaded the data intelligence platform was also able to comment and provide ai summaries around what that data actually is this democratized is used for our analysts who sometimes struggle to figure out where the correct data source lies finally we've also built hundreds of ml and ai models on this data the ml registry governed by unity catalog gives us a great platform to organize and search those models however unity catalog also allows us to govern who and which data teams can access models and features from the feature store for their own projects data lineage of all of this gives us a great insight to see how the data flows from source to modeling to those final bi reports that our players need transparency builds trust and of course data sharing allows us to connect with other data verticals and vendors inside of our department this includes ballpark it includes concessions as well as sharing live data within how the fans are engaging with the team everything with the appropriate permissions in place the net result we now have four times more data ingested and used for ai at the same cost as our legacy systems we have hundreds of users scattered around the country and the globe with secure and governed access to these data and ml kpis we also have 10 times faster data insights after games and workouts getting the reports into the hands of our players quickly that they need to be the best that they can be and of course all of this contributed to our first ever world series win i tried to have a spotlight on my ring for the whole time but it's a it's all they didn't said no to that but Databricks is really helping our organization win by impairing our team with data intelligence however we're just getting started here with the rise of generative ai we have invested time and effort to find innovation in this new space i actually have a quick demo where i will be using the Databricks ai bi genie to provide a natural language interface into our data with the trade deadline coming up as well as being in san francisco i thought it would be fun to see if there are any players on the san francisco giants that might have future trade value in this application we are using public data from baseball savant notice that these tables as well as the application are both governed throughout unity catalog users need to have the correct permissions to access both comments as well as summaries describe and help teach the genie application what exactly these internal kpis that mean something to me but maybe not to you what those actually are and of course all of this needs to be shared and governed within the workspace analysts can ask broad questions of this data you're going to see here that we're going to be looking for just who on the giants has any trade value i know i type super slow i guess they're were like do you want to do something the computer and i'm like it's fine you can pretend that i'm typing that out uh the genie doesn't know how to answer this question of what is trade value it just brings back statistics about players on the giants so what we can do now is instruct the genie what i care about with trade value i want to look at the difference between expected and observed performance to look for undervalued players notice that we quickly see that the genie application is able to see that luis matos as well as matt chapman both have had significant ific underperformance this season on the giants but maybe we'll have better performance for the rest of the season if they call louisa mat ever back up from triple a but that's a side note anyway we can save this as a thumbs up as well as provide instructions and save it as an instruction to save it off as uh for an easy access later on and we can also visualize this data for quick consumption since we saved this as an instruction it's trivial now to do the same analysis for other baseball clubs here i'm asking it do the same anal anis for the chicago white socks after some time thinking that's my double fast forward click there it goes we see that martin maldonado as well as andrew ben attendi have been underperforming for the chicago white socks what this has allowed us to do is democratize and allow our analysts sql developers and less technical stakeholders unprecedented access to our raw data in our database this allows them to ask the questions they need and create an efficient starting point for targeted and further decisionmaking leading into the trade deadline thank you so much to the Databricks team that supports us michelle hussein chris that onboarded us up here thank you for the opportunity to speak with you all this morning finally we're always looking to continue pushing the boundaries of data and ai in sport if interested please reach out baseball is a team sport after all and i will say we do a lot of our hiring in the offseason so best of luck if you use that qr code but you can always find me on linkedin and happy to kind of talk about this further thanks so much to [applause] [music] [applause] [music] [applause] everybody wow that's so cool did you guys see the ring that he was wearing was like gigantic uh that's so awesome uh i call it moneyball 2.0 uh you got to go check out their booth in the expo hall they can actually analyze your swing and everything like they'll collect all the data and they'll you know uh give you a score and you can improve it so check that out okay so i'm going to introduce nex uh my co-founder um i actually said it backstage that you know they said hey he's the number one committer in apache spark but actually we looked and now he's actually no longer he's number three but for seven years uh he was the number one committer on apache spark project uh and he's going to tell us about spark and one of the cool things is you know this project is now you know 10 years plus old uh so you think that okay we know what spark is but actually it has dramatically changed in the last just two three years okay the project has completely been transformed and it's going to tell us how we did that and what are the changes and how did the community uh pull off this change that it's gone through so let's welcome uh rold chin to [music] stage all right thank you ali for that uh number three speech uh good morning again um so as many of you know this conference actually started previously named as the spark summit or spark nii summit and in this talk we'll be going back to the roots of the original conference apachi spark um so three years ago at this conference we pull a 100 of you and we ask what were the biggest challenges you had was apachi spark all right so about 100 of you and here's what the 100 you told us by far the number one was hey i have a bunch of scala users they're in love with spark it's great but they also have a whole bunch of python users out here as a matter of fact there's way more of them and there really doesn't get spark sparks kind of clunky it's difficult to use in python um it's not a great too for them and the number two is everybody else was said hey um i love spark i've been using it i'm using scalar also but dependency management or my spot application is a nightmare and version upgrades are takes 6 months one year 3 years you name it um and then there's a consensus among the language framework developers out there not a huge population but very important component of the apach spark community would tell us hey because of that tight jvm language uh sort of nature of spark it's very very difficult interact with spark outside of jvm as a framework developer not just as an end user so we got to to work um and let's talk about the first one spark scala but my usual is only python all right if you've been to this conference uh in the past you know this is not the first time um we're talking about python but i found this video from about three years ago um just the other day as i was preparing the talk and it's from zach wilson who used to be a data engineer airbnb and here's what zach has to say another one is spark is actually native in scala so writing spark jobs in scala is the native way of writing it so that's the way that spark is the most likely to understand your job and it's not going to be as buggy so zach scal i believe zach's actually maybe somewhere sitting here um but scala is the native way of riding spark and writing is not as buggy so it's not just that people at this conference saying that right we got to work um 3 years ago at this conference i think it might have still be named sparking a submit back then and the uh theme of all the slides were white background instead of dark background we talked about the project zen initiative by the apachi spark community and it really focused on the holistic approach to make python the first class citizen and this includes api changes including better error messages debuggability performance improvements you name it right comeo almost every single aspect of development experience um 2022 two years lat two years earlier uh we gave a program report and talked about all the different improvements we have done in those two spark releases and last year we show a concrete example of how much auto complete have changed just out of the box from spark 2 all the way to spark 3.4 so um in this slide summarizes um a lot of the key important features for p spark in spark 3 and spark 4 and if you look at them it really tells you that python is no longer just a so bodon on the spark uh but rather first class language and there's actually many python features that are not even available they're python made python idiomatic they're not available in scala for example you can define python usually define uh table functions these days and use that to connect to arbitrary data sources it's actually much harder thing to do in scala at this conference alone this year we had more than eight talks talking about various features of just py spark itself so a lot of work have gone into it um but how much benefit are the users seeing um again this is one of those moments i would tell you i can tell you non-stop about it but it's the best you try it out yourself it's actually completely different language um when you look at the last 12 months alone um pypar has been downloaded by over 200 countries and regions um in the world just according to piie stats and i was doing some number analysis the other day i was really surprised to find this number um just on Databricks alone for spark versions 3 3 and above so it does not include any of the earlier spark version which is's a lot of them out there but just for spark 3.3 versions and above on Databricks um our customers have run more than five billion ppar queries every day me to give you a sense of that scale um the i think the leading cloud data warehouse runs about five billion queries a day on sql um this is actually matching that number it's only a portion a small portion of the overall p spark workloads um but the coolest thing was as i found the earlier video from zach um in which he said hey scala's the native way of doing it i found another video he published just about three months ago by the way i've never met jack zach until like last week when i uh call him reach out to him ask hey would you be okay for me to show you the video but let me play you this video from this year by zach uh but things have changed in the data engineering space uh the spark community has gotten a lot better about supporting python so if you were using spark the differences between pie spark and scola spark in spark 3 are there really isn't very much difference at all so thank you for the endorsement from zach right so if your impression of spark was hey um spark is written natively in scala that's still true we love scala but your impression is hey if i'm really using python i will get super crazy jvm stack traces i get terrible error messages the api idiomatic try it out again it looks completely different from three years ago right and of course the job's never done we'll continue working on improving python for spark uh but i think it's fairly uh so reasonable declare hey python is a first class language of uh spark so now let's talk about the other two uh proms version upgrades dependency management and jvm language um now let me dive into a little bit more about why this prts exist so the way spark is designed is that all the spark application you write your etl pipelines your data science analysis tools your notebook uh logic that's running um runs in a single monolithic process called a driver that includes all the core server sides of spark as well so all the applications actually don't run on whatever clients or um servers they independently run on right running the same monolithic server cluster and this is really u sort of the essence of the problem because one all this because they all run in the same process the application have to share the same dependency and not only do they share the same dependency each other they share the same dependency as spark itself um debugging is difficult because in order to attach a debugger you have to attach the very process that runs all of the things um and now last but not least if you want to upgrade spark you have to upgrade the server and you upgrade every single application r on the server in one shot it's all nothing and this is a very difficult thing to do when they're all tightly coupled so two years ago at this very conference u martin and i introduced to you spark connect um the idea of spark connect is again very very simple at a high level want to take the data frame and sql api um of spark that's either python scala centric and crea a language agnostic binding for it based on grpc and ap arrow and this sounds like a very small change because it's just introducing a new language binding and new api lric agnostic but really it's the largest architectural change to sparks since the introduction of data frames apis themselves and with this language agnostic api um now everything else run as clients connecting to the language agnostic api so we're breaking down that monolith into you can think of it as microservices running everywhere and how does that impact endtoend applications well different applications now will actually run as clients connecting to the server but there are really clients that're running in their own sort of isolated environment and this makes upgrade super easy because the language binding is designed to be or bindings designed to be language diagnostic and for and backward compatible from api perspective so you could actually upgrade the spark server side say from spark 3.5 to spark 4.0 without upgrading any of the individual applications themselves and then you can upgrade applications one by one as you like at your own pace same thing with debuggability now you can attach the debugger to the individual application this runs in a separate process anywhere you want without impacting the server without impacting rest of the applications now for all of the language developers out there um this language agnostic api makes it substantially easier to be building new languages just in the last few months alone um we have seen s of community projects that build gold bindings rust bindings shop bindings all of this and it can be built entirely outside the project with their own release cadence so one of the most popular programming languages probably the top two post uh programming languages for data science um are r and python right spark has built-in python support there also built-in r support spark r but the actually the the most popular art programming uh library for spark is not the built-in spark art it's a separate project called sparkly art and sparkly art is made by this company called posit um which is actually i i was talking to uh the posit folks uh behind the stage and uh i told them hey i think p is the coolest open source company audience i never heard of and the reason you have not heard of it is they rename themselves fairly recently to posit um but the people at posit created the most foundational open source projects for example deer the very project that define the grandar for data frames that we're all enjoying today ggplot the visual grammar of visualization our studio the most popular r ide um west mckin who created panda works at posic um also apachi arrow um so i would actually like to welcome tarf president of posit onto stage to talk to you more about sparklyr [music] [applause] [music] good morning everyone thank you very much for the introduction it's very kind of you we uh i'm very excited to be here and thank you Databricks for giving us the opportunity to uh to speak to this audience we as a company are probably somebody people that you don't know you've never heard of us until uh he gave you a little bit of update but we are a public benefit corporation we've been around for about 15 years our focus is very much about code first data science um the uh our governance structure is one that allows us to think about things for a very very long term so our ambitions are actually to be around for the long haul and to continue to invest in these open source tools we have been we support hundreds of our packages and we also support the r studio ide and if you if you've been watching us for a while you may have noticed that over the last 5 years we've added a lot of capabilities to the python ecosystem right so uh li in some cases these are multi multilingual uh solutions so things like quto shiny for python great tables all of these are examples of of projects that we have and we have more that are coming out over the coming years the um sorry i'm having a hard time reading this slides in 2016 we released a package called sparkly r and the reason we released it is because we wanted to have an idiomatic implementation for uh for for the r users that is more aligned with what the tidy verse is and for those of you who don't know what the tidy verse is it's like a philosophy of how you write packages and uh and the patterns that sort of go along with that the original design of spark made it so that for users in corporations in particular to be able to use it they would have to run our studio and r on the servers themselves and so you can imagine when spar when spark connect became available last year we were very very excited because it finally solved one of the key problems that we saw which is like how do you make it so that the end user through a client does not have to get into a jbm can just access it directly and so happy to say you know about so we started last year and basically by the end of last year we had had gotten uh support for sparkk connect to happen uh unity catalog we worked with the Databricks team to figure out how to make sure that sparkly r and the ide had uh clean support for that and one of the most interesting things is we added support for our userdefined functions which is actually a really big deal because now the r users in the in the in your organizations can actually participate in using spark to solve the really hard problems and they can collaborate with other people in the sparkk ecosystem so we're very excited about that and we're interested in sort of getting people feedback on that if you get a chance to try it out so this is very anticlimactic those of you who were there yesterday for the demo you saw casey she like the world stopped we decided to make life easy it's hard to demo some of these things but the change this is the open source desktop ide and you can see that that's this oneline change that you have to make to be able to connect to spark connect and now this user on the desktop can go ahead and access the spark cluster and leverage the full capabilities of that this is one of the key things that we're we think that make a big difference in terms of people's ability to contribute and adopt spark so you probably have noticed we we we've over the last year we've been announcing all kinds of things with Databricks one of the key things obviously where sparkly are and uh spark connect and support for that but we have also uh been making changes on our commercial product so the first commercial product that we have supporting this is something called posit workbench which gives you a sort of a server-based authoring environment that uh supports our studio jupyter notebook jupyter lab vs code and ties into the authentication and authorization of the systems and so you basically get the full power of the governance that you have in Databricks but having it surfaced to their data scientists you can expect that over the coming year you'll be more commercial products and open source tools that will have those tighter tighter integrations with the datab break stack if you're at all curious or interested you know feel feel free to check out any of these links to he'll learn about how we're working more with spark and spark connect and how we're working with Databricks thank you very [music] [applause] much all right thank you tarf the reason i'm so excited about spark connect is that it makes like uh the frameworks like sparkly are possible it makes it easy to use makes it easy to adopt easy to upgrade easy to build and this really ultimately benefits all the developers all the data scientists all the data engineers out there because now they can use whatever language they're most comfortable with it doesn't require all of those to be built into spark you will get idiomatic r on spark now um with spark connect it's really trying to solve this last two prs version upgrades managing spark make it easier to be building non jbm language bindings with that it brings us to spark 4.0 this is actually not a conference in which we will announce spark 4.0 is really today it's actually upstream open source project working at its own pace but it is coming later this year to give you a preview of some of the features just similar to other major version previous major version releases of apach sar there will be thousands of features i can't possibly go all into today but spark connect will ga um and become the standard in spark for um nc sql will become the standard in spark for there's a lot of other features that we're looking forward to but one thing i'm particularly excited about as at um definitely at this conference is that the um the opportunity for the different open source community to be collaborating with each other um especially when it comes to computer and storage so many many features actually requires co-designing the compu stack which is where apachi spark comes in as well as a story stack which is where delta lake l foundation delta lake and apachi iceberg come in um as a matter of fact many of the features you've heard about at this conference at session talks at ken notes collations road tracking merge performance variant data type sean talked to you about um type winding there are not just features in delta or features in iceberg features in spark they actually require coth thinking about all three projects um for them to work and this is s of really a spirit um of open source and the spirit of collaboration in open source so last week even though spark 4.0 is not officially released yet last week the apart spark community have uh officially released spark 4.0 preview it's not the final release but it gives you a glimpse into what spark 4 would look like please go to the website check it out download it give it a spin and let us know your feedback thank you very [applause] [music] much awesome super excit about spark 4.0 um i got to say you got to check it out p spark is amazing these days and then also all that version management installing it uh managing spark it's just so much simpler these days uh i just tried it uh a week ago you can just go to any terminal and just say pip install pie spark and that's it it'll just install the whole thing it just works it's hugely different from let's say 10 years ago where you would have to set up the you know the servers and the demons and all of that and configure it and use it in local mode and all that just pip install pie spark okay so uh back to our data intelligence platform road map we're now reaching towards the end uh but this is the most exciting thing for me uh this this project that we're going to talk about next uh is something that actually a couple years ago we asked all the top cios that use Databricks what's the number one thing you want Databricks to do and it was a really surprising answer it was something that we didn't expect them to say to do and then since then we've been super focused on nailing this pro problem so so i'm very very excited to welcome on stage belal aslam who are going to take us through what we've done there let's welcome [music] him all right good morning so i i'll get started as it turns out there are five bals at Databricks i asked all five to give me a little cheer but that was more than five so thank you uh okay so thank you ali for the introduction so we've heard about machine learning we've heard about bi we've heard about analytics and all these amazing things and i'm here to tell you that every single one of them everything starts with good data all right how do you get to good data well there are three steps you have to follow uh and every single one of us including me we are traditionally cobbling together lots of different tools in an ever increasing tool chain that gets more and more expensive more and more complex let's go through that real quick so spark and especially Databricks is already very good at big data as rold was telling you this is the world's biggest big data processing framework but as it turns out that a lot of your really valuable data is sometimes in smaller data so for example you may have mysql oracle postgress all these different databases they're incred incredibly valuable so you might be setting up deum and kafka and a monitoring stack and a cost management stack just to get the changes from these systems into Databricks or you might i'm actually pretty confident that every single one of us is using a c crm of some kind maybe you're using salesforce nets suite maybe you use hrms like workday and netsuite right tons of valuable data in there just waiting to get into Databricks so you can uh start using it and then once your data is in a data platform like Databricks the next step the very next step is is to transform it as it turns out newly ingested data is almost never ready for use by the business you have to filter it you have to aggregate it you have to upend it and clean it lots of technology choices cbt a great open source project you might have heard about delta lif tables in pisp park reyal was telling you how popular it is which one of these do you use and again how do you monitor it how do you maintain it and once your data is transformed that's really not even half the battle you get the value out of data by actually running your production pipelines in production i don't like waking up at 2 in the morning with an alert so now you have to orchestrate so you might be using airf flow great now your tool chain just expanding just a little bit more you're responsible for managing airl flow and its authentication stack and so forth and then of course you might have to monitor all these things in cloudwatch this is unnecessarily complex and this is inefficient and it's actually very expensive which is why i am extremely proud to unveil what we're calling datab brees lake flow this is a new product that's built thank [applause] you this is a new product that's actually built on a fundamental foundation of datab breaks workflows and delta lif tables with a little bit of magic sauce added on and i'm actually going to start with the magic sauce and it gives you one simple intelligent product for ingestion transformation and orchestration all right let's get into it the very first component of these three components is something we call lake flow connect lak flow connect is native to the lake housee these are native connectors and when i say they're native and they're high performance and they're simple for all these different enterprise applications and databases if in the audience today you're using server postgress a legacy data warehouse or using these enterprise applications we're on a mission to make it really simple to get all of this data into Databricks and this is actually powered by aron technology a company we acquired last year um so i'll give you a quick demo in a moment but i actually want to talk about one of our customers called insulet and insulet manufacturers in a very innovative insulin management system called the omnipod and they they had a lot of customer support data locked up in salesforce and they're one of our customers of lake flow connect with that they're able to get to they used to spend days on getting the insights now they have it down to minutes it's super exciting all right so actions speak louder than words so let's take a look all right so you're in lake flow here and what i'm going to do is i'm going to okay i'm going to click on ingest and you see it's it's point and click which is pretty awesome and it's designed for everybody i'm going to use click on salesforce and my friend eric oren has set up a connection by the way everything in lake flow is governed by unity catalog and it's secured by unity so you know you can you can manage it very easily and govern it and there are three steps and now okay great so now i see these objects from salesforce and uh i'm going to choose orders i actually work for casey i don't know if you remember her cookie company i'm building the data pipeline she's my ceo so i'm going to bring in some order information uh for our ever growing cook business into the into this uh catalog and schema and hang on a second there we go and within seconds data should show up in our lake house excellent all right that's it that's all it took there are no more steps all right let's get back to slides so i want to do something here and going to give you a peek behind the curtain uh we're all engineers here and there's actually something pretty magical that's happening in out side of lake flow connect you might think you know gosh how hard could it be to connect to these apis and these databases you know can't you run a sql query it turns out that what you actually want to do is only obtain the new data the change data capture from these source systems from these databases and enterprise applications and as it turns out this is a really really hard problem you don't want to tip over the source database you don't want to exhaust api limits the data has to arrive in order it has to be applied in order things go wrong it's the the real world and you're coupling systems together and you have to be able to recover all of this is undifferentiated heavy lifting and i'm really glad that we're doing it because with archon tech cdc is no longer a three-letter word it's point and click it's automated operations and it's consistent and reliable super [applause] exciting all right let's go to the second part of this product the second component what happens once you bring in data you're now able to load data from these databases and enterprise applications the very next thing you have to do is to transform it which is to prepare it remember you have to filter aggregate join and clean it typically this involves writing really complicated programs that have to deal with a lot of airor conditions and i'll show you that in a moment uh the magic trick behind lake flow pipelines because it's built on the foundation it's the evolution of delta life tables is that it lets you write plain sql to express both batch and streaming and the magic trick is we turn that into an incremental effic and cost effective pipeline okay so let's go back uh and remember i am making i i just did some ingestion of data and what i'm going to show you is i've also pulled in data from sql server i won't show you that flow so i have data from salesforce i have data from sql server and i need to now go ahead and create a little bit of an aggregation out of that okay so let me show you how simple that is within lake flow one of my favorite features here by the way is that it's one single unified canvas so this little dag at the bottom you always see it you can hide it if you want but i'm going to click here on salesforce and i'm going to write a transformation okay that's simple now this is an intelligent application it's built on the intelligence data intelligence platform so i might just go ahead and ask the assistant what it thinks i should join uh okay it comes up with a pretty reasonable join it says you can join these tables i'm just going to let it figure out how to join them figure out the key for me and that's pretty awesome okay that looks about right it found the customer from id key i'm going to go ahead and accept that and let me just run this transformation real quick i don't have to deploy it i can run it in development and it'll actually give me the ability to debug it real quick okay perfect so i can see that i have orders dates products customers all of this came together really nicely i have a nice little sample of data perfect all right so we can go back to slides now okay thank you that was that was one of the bals uh okay great so let's again i'm going to give you a little bit of a peak behind the curtain here why is this pretty amazing notice that in in this there was no cluster there was no compute i didn't have to set up infrastructure i didn't have to write a long script i just wrote sqil and this is the power of declarated transformations this here is actually my valuable transformation and instead without lake flo pipelines you have to do table management you have to do increment alization many many times and even have to deal with schema evolution i've spoken with some of our customers they've written entire frameworks to do schema evolution and schema management again that's undifferentiated heart heavy lifting why should you spend time on that right and this this beast just grows and goes and lak flow pipelines are powered by something called materialized views and they're magical because they automatically handle the bookkeeping for you they handle schema evolution they handle failures and retries and back fills and they magically choose the right way to incrementalization so in my world my ceo my cookie ceo is really demanding our e-commerce website is just really taking off and now we will need to be able to do uh real-time actions on our website so from this pipeline this which pipeline which looks like batch i'm going to add some streaming to it i'm going to go ahead and write some uh joined and enriched records into kafka so let's let me show you how easy that is and let's take a look at that great so this remember this is my pipeline here uh i did i did this materialized view a transformation so again from this unified canvas i just add a pipeline step and i'm going to go live here i'm going to write some code okay so i'm going to create something called a sync think of a sync as a destination i'm just going to call it kafka because i'm going to write to kafka and all i have to do is this is kind of cool because all i'm doing is writing sql here and i'm going to point this at ca. dates.com and that should be enough to create a sync and all the credentials are coming through unity catalog so this is again governed and i'm going to create something called a flow and a flow think of that as an edge that writes changes into kafka i'm going to do target kafka and i'm going to select from the sales table that i just created and i'm going to use the table changes table value function um okay something's not right here and i need to do dev great okay so this looks good and remember this is what looks like a batch pipeline and i'm going to turn this into streaming there we go and just like that our data is in kafka let's go to [applause] slides and this is something super exciting there is no code change here i didn't have to make a change i didn't have to choose another stack everything just works together one of my one of the coolest things we're doing is something called realtime mode for streaming you can think of this as realtime mode for streaming makes streaming go really really fast and the magic trick here it's not fast once or twice it's consistently fast so if you have operational streaming use case where you have to deliver data and insights just turn it on and this pipeline will go really really fast and we have talks about it ryan n house is doing a talk on it so please go check it out perfect so now i have i've ingested data from sql server and salesforce i've very quickly built a pipeline that is able to deliver badge and streaming results is always fresh i didn't have to do manual orchestration but now my ceo is very demanding the cookie business continues to grow and casey wants insights and she wants a dashboard that she can use to figure out how her business is doing and this is where orchestration comes in and orchestration is really how do i do all the other things that are not involved with loading data and transforming data such as building a dashboard and running it or refreshing a dashboard one of my favorite capabilities in Databricks is something called Databricks workflows and we've evolved it into the next generation and workflows is a drop in complete orchestrator no need to use airf flow airf flow is great but it's completely included in Databricks and this is just a list of innovations it has lots of capabilities that you might be used to in traditional orchestrators okay so what i'm going to do here now is i'm going to walk over and i'm going to start building a dashboard i'm going to to run it after my pipeline is done okay let's take a look okay so remember i have data going into kafka i have all this i'm going to just add another step i love this unified canvas it's like a really nice context on where i am and this is super cool the just assistant suggest a dashboard that's pretty cool actually useful revenue and product insights i like that that's what i would have wanted and uh let me hide that a little bit and there it is that's our dashboard so hey good news our cookie business continues to grow we're not all the way done with the business and this is super cool uh we actually have a really interesting insight here that sugar cookies tend to sell in in the month of december so super cool so that's it you don't have to do anything else uh let's get back to [applause] slides so i'm going to wrap up really quickly uh i'm super excited about one innovation that i think will make our lives as data teams and data engineers much much better look it's great to create dags things that run after another it's great to have schedules when should something run but as your organization grows what you really want are triggers and triggers think of them as work happens when new data arrives or data is updated and this is is actually what allows you allow allows us to do another magic trick which is run your pipeline exactly when it's needed when upstream tables are changed when up upstream files are ready this is super cool it's in completely available in the product it's actually a foundational block of uh lak flow jobs perfect so now my everything is running i've ingested data i have transformed it i've built a dashboard my pipeline's running in production like i said i hate waking up in the middle of the night and typically i have to glue together a lot of different tools to see cost and performance lake flow includes unified monitoring includes unified monitoring for data health data freshness cost runs and you can debug to your your heart's content but it has that single piece pane of glass so you don't have to if you don't want to lake flow is built on the Databricks intelligence platform it's native to it this gives us a bunch of superpowers you get full lineage through unity catalog that includes ingested data so all the data upstream from salesforce or workday or mysql we already captured the lineage it includes federated data it includes dashboards even ml models not a single line of code needed it's built on top of serverless compute frees you up from managing clusters managing instances which how many executors what type of instent should i use it's serverless it's secure and completely connected to unity so it fleas you up from that hassle but what's also really cool about it is that we did this benchmark this is real data for streaming inest it's three and a half times faster and it's 30% more cost effective so that's you know have your cake and eat it too it's super exciting [applause] data intelligence is not just a buzz word as you have seen in the last couple of days it's actually foundational to Databricks it's also foundational to lake flow lak flow includes a complete integration with Databricks iq and the assistant so every time you're writing code every time you're building a dag every time you're ingesting data we're here to help you uh author monitor and diagnose and one last thing this is actually an evolution of our existing product so you can confidently keep using delal life tables and workflows we'll make sure that everything is forward is backwards compatible all your jobs and pipelines will continue to work and you can start enjoying lake flow so lake flow is here uh we're actually doing a talk elise and peter are doing a talk on uh lake flow connect i think very soon lak flow connect is in preview please join us give us feedback on what connectors you want we're very excited about it pipelines and jobs are coming soon all right i think that's it thank [applause] [music] you awesome all right that was awesome bal ivth sounds like a king uh that was super super awesome what i really loved about that is i don't know if you noticed it this is actually a big deal so spark has micro batch architecture so things take you know in when you're trying to stream things it takes a couple seconds sometimes 5 six seconds what he showed you realtime mode that we now have real time mode gets it down to 10 20 milliseconds so it's like a 100x improvement the p99 percentile of uh latency is you know around 100 millisecond so it's kind of game changer and then of course we saw connect you can get your data in there you can do incrementalization you don't need to worry about you know getting the logic right it'll just do it for you you so super exciting okay awesome so i just want to wrap this up quickly so on the top row there you see the announcements from yesterday i'm not going to bore you and go through those again uh on the bottom row you can see what we did today so we just heard data engineering so you saw that unity catalog open source live on stage by mate that was super cool but also metrics i'm excited about metrics every company has kpis how do we have certified kpis that we can rely on and that we know are semantically correct and we know how to compute them so that's also a big deal and then we heard about al lake 4.0 and project uniform going ga so lots and lots of great stuff and that's it for today uh hope you enjoy your lunch and then please go to the sessions they're super super awesome thank you everyone thanks \n"
     ]
    }
   ],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, WordCloud Keyword Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove STOP WORDS like \"at\", \"the\" and \"and\" - then create a word cloud by frequency of key terms in the transcript\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Download dics of words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(transcripts)\n",
    "\n",
    "# Filter out stop words and junk words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "more_stop_words = ['use', 'well', 'azure', 'new', 'one', 'know', 'see', 'okay', 'got', 'make', 'actually', 'let', 'sort', 'yeah', 'able', 'need', 'going', 'show', 'take', \\\n",
    "                   'microsoft', 'thomas', 'aws', 'help', 'gon', 'na', 'back', 'us', 'um', 'uh', 'way', 'right', 'much', 'really', 'want', 'get', 'google', 'amazon', 'also', 'like' \\\n",
    "                   'thank', 'like', 'thank', 'great', 'including', 'using', 'applause', 'today', 'across', 'find']\n",
    "all_stop_words = stop_words.union(more_stop_words)\n",
    "keywords = [word.lower() for word in words if word.isalpha() and word.lower() not in all_stop_words]\n",
    "\n",
    "# Calculate freq. dist. of the words remaining 'keywords'\n",
    "freq_dist = nltk.FreqDist(keywords)\n",
    "\n",
    "# Get the top N most common, after remove basic words, replace (N) w/ the TopN you want.\n",
    "common_words = freq_dist.most_common(10)\n",
    "\n",
    "# Create a string\n",
    "word_freq = {word: freq for word, freq in common_words}\n",
    "\n",
    "# WordCloud (WC)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                      background_color ='white', \n",
    "                      min_font_size = 10)\n",
    "\n",
    "# Generage the WC\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot and show the WC           \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud)\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
